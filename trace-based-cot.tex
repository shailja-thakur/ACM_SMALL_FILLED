%%
%% This is file `sample-acmsmall-conf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,acmsmall-conf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-acmsmall-conf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[acmsmall,screen,review,anonymous,nonacm]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation email}{June 03--05,
  2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/2018/06}

\usepackage{tabularx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{amsmath}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
\usepackage{array}
\usepackage{subcaption}
\usepackage{natbib}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage[most]{tcolorbox}
\frenchspacing
\let\Bbbk\relax
\usepackage{bibentry}
\usepackage{amssymb}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{pifont}
\usepackage[dvipsnames]{xcolor}
\usepackage[table]{xcolor}
\definecolor{highlightgray}{gray}{0.85}
\usepackage{placeins}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage{algorithm}
\usepackage{algorithmic}
\newcommand{\gcmark}{\textcolor{ForestGreen}{\cmark}}%
\usepackage{newfloat}
\usepackage{listings}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Let the Code Speak: Generating Verifiable CoT from Execution-Traces}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Ben Trovato}
\authornote{Both authors contributed equally to this research.}
\email{trovato@corporation.com}
\orcid{1234-5678-9012}
\author{G.K.M. Tobin}
\authornotemark[1]
\email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{Institute for Clarity in Documentation}
  \city{Dublin}
  \state{Ohio}
  \country{USA}
}

\author{Lars Th{\o}rv{\"a}ld}
\affiliation{%
  \institution{The Th{\o}rv{\"a}ld Group}
  \city{Hekla}
  \country{Iceland}}
\email{larst@affiliation.org}

\author{Valerie B\'eranger}
\affiliation{%
  \institution{Inria Paris-Rocquencourt}
  \city{Rocquencourt}
  \country{France}
}

\author{Aparna Patel}
\affiliation{%
 \institution{Rajiv Gandhi University}
 \city{Doimukh}
 \state{Arunachal Pradesh}
 \country{India}}

\author{Huifen Chan}
\affiliation{%
  \institution{Tsinghua University}
  \city{Haidian Qu}
  \state{Beijing Shi}
  \country{China}}

\author{Charles Palmer}
\affiliation{%
  \institution{Palmer Research Laboratories}
  \city{San Antonio}
  \state{Texas}
  \country{USA}}
\email{cpalmer@prl.com}

\author{John Smith}
\affiliation{%
  \institution{The Th{\o}rv{\"a}ld Group}
  \city{Hekla}
  \country{Iceland}}
\email{jsmith@affiliation.org}

\author{Julius P. Kumquat}
\affiliation{%
  \institution{The Kumquat Consortium}
  \city{New York}
  \country{USA}}
\email{jpkumquat@consortium.net}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  Teaching language models to reason about code execution remains a fundamental challenge. While Chain-of-Thought (CoT) prompting has shown promise, current synthetic training data suffers from a critical weakness: the reasoning steps are often plausible-sounding explanations generated by teacher models, not verifiable accounts of what the code actually does. This creates a troubling failure mode where models learn to mimic superficially convincing but logically flawed reasoning patterns.

  We address this by grounding CoT generation directly in program execution traces. Our pipeline instruments code to capture its dynamic behavior, then narrates these verified execution traces into natural language rationales that are correct by construction. This execution-grounded approach ensures every reasoning step reflects what the program genuinely computes, eliminating logical hallucinations at the source. We evaluate our method on code reasoning tasks (forward reasoning on CruxEval and LiveCodeBench-Exec, backward reasoning on CruxEval-Input), as well as code generation and explanation tasks from HumanEval. Models trained on our bi-directional trace-grounded data achieve substantial improvements, with gains of up to \textbf{30 points} on output prediction and \textbf{28 points} on input prediction over base models, alongside improved explanation accuracy and modest gains on code generation, demonstrating that verifiable reasoning fundamentally enhances model capabilities. The complete synthesis pipeline will be released as open-source.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>00000000.0000000.0000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Do, Not, Use, This, Code, Put, the, Correct, Terms, for,
  Your, Paper}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
The remarkable success of Large Language Models (LLMs) in generating syntactically correct code often masks a
     deeper challenge from a programming languages perspective: understanding program \textit{semantics}. While LLMs
     are powerful static models of source code, they fundamentally lack a connection to the \textit{dynamic behavior}
     of programs—how state evolves during execution. This gap limits their utility to that of sophisticated
     auto-completes rather than true reasoning partners for tasks like debugging and program analysis, which require a
     faithful understanding of a program's operational semantics.

% To improve these reasoning capabilities, the research community has widely embraced fine-tuning models on Chain-of-Thought (CoT) data~\cite{wei2022chain}, which provides explicit, step-by-step rationales. However, sourcing this data for the domain of code is particularly challenging. Recent self-instruct methodologies use a powerful "teacher" model to generate massive datasets of code and explanations, but these suffer from a fundamental flaw: \textbf{the generated reasoning steps are not tethered to any ground truth}. The teacher model's explanation is often a plausible "hallucination" of the code's logic~\cite{turpin2023languagemodelsdontsay,beger2025coconutstructuralcodeunderstanding} rather than a factual account of its execution, leading to models being trained on subtly incorrect rationales.

To bridge this gap, the research community has embraced fine-tuning models on Chain-of-Thought (CoT) data, which
     provides explicit, step-by-step rationales to guide the model's reasoning process. However, the methods used to 
     generate this data for code are often \textit{unsound}. A powerful "teacher" model generates the rationale, but 
     these reasoning steps are not tethered to a program's actual execution trace. The result is a dataset of "logical 
     hallucinations"—plausible but factually incorrect accounts of the program's logic. This lack of guaranteed logical
     integrity is a critical barrier for high-stakes applications. For \textbf{debugging}, a rationale that 
     misrepresents the state of variables is actively misleading. For \textbf{iterative self-refinement}, feeding a 
     model its own flawed reasoning can cause compounding errors. For \textbf{agentic workflows}, a plan based on an 
     unsound understanding of a previous step can derail an entire task.

% This challenge is particularly acute in code reasoning because small logical errors can cascade through multi-step reasoning processes, leading to flawed understanding. For instance, a model might correctly identify that a function performs a sorting operation but incorrectly reason about the intermediate steps, variable updates, or edge case handling. Such errors in reasoning data propagate during training, teaching models to follow superficially plausible but ultimately incorrect logical patterns. For instance, Other 
%      execution-aware methods, used in benchmarks like HumanEval~\cite{chen2021evaluating} or by prior work like 
%      SemCoder~\cite{ding2024semcodertrainingcodelanguage}, only perform \textit{outcome verification}. They check that 
%      the final result of a program or rationale is correct, but they do not verify the logical integrity of the 
%      intermediate reasoning steps. This leaves a critical gap for a method that can guarantee the soundness of the 
%      entire reasoning chain.
% This challenge is especially acute in code reasoning, where small logical errors can cascade into a flawed understanding. For instance, a model may correctly identify a function's purpose but hallucinate the logic for its intermediate steps or edge cases. Such errors then propagate during training, teaching the model to follow plausible but incorrect logical patterns.

Other related lines of work use execution to evaluate model outputs, not to generate training data. For instance, benchmarks like HumanEval~\cite{chen2021evaluating} and MBPP~\cite{austin2021program} rely on executing generated code against unit tests to verify the correctness of the final answer. This validates the outcome but provides no insight into the intermediate reasoning steps. Similarly, highly relevant methods like SemCoder~\cite{ding2024semcodertrainingcodelanguage} generate CoT explanations for fine-tuning, but their execution-based verification is limited to the final outcome, leaving the logical integrity of the intermediate reasoning steps themselves unchecked. \textbf{Two critical gaps therefore exist: a lack of methods for generating verifiably correct CoT steps, and a lack of pipelines that provide fine-grained control over the complexity and diversity of synthetic data.}

Furthermore, effective code reasoning requires both forward inference (predicting outputs from inputs) and backward reasoning (understanding how outputs arose from inputs)~\cite{li2025codeio}. This bi-directional capability is essential for debugging, code comprehension, and robust program analysis. However, current synthetic data generation methods do not systematically address this requirement, leading to models with asymmetric reasoning capabilities.

In this work, we bridge these gaps by introducing a new paradigm for data synthesis that is grounded in program
      execution. We have developed a complete synthesis pipeline that generates CoT rationales by \textit{narrating
      verifiable program execution traces}. Specifically, we instrument a program to capture its dynamic execution
      state (e.g., variable values after each statement) and use this log as an immutable ground truth. This ensures
      that the resulting reasoning chain is \textbf{correct by construction}, eliminating the logical hallucinations
      that plague other methods.

Furthermore, our \textit{concept-first} pipeline provides fine-grained control over data quality. Rather than
      relying on existing code corpora, it synthesizes problems from abstract PL and algorithmic concepts (e.g., 
      "pass-by-reference semantics," "dynamic programming") extracted from technical literature. When combined with our
      generation of bi-directional traces, this approach provides a complete foundation for training models on both 
      predictive forward-reasoning and diagnostic backward-reasoning, grounded in the same verifiable execution.


Our contributions are fivefold:

\begin{enumerate}
    \item \textbf{An Execution-Grounded Synthesis Pipeline} that generates factually correct and verifiable CoT data directly from execution traces.
    \item \textbf{A Concept-First Generation Approach} providing fine-grained control over the complexity and diversity of the synthesized data.
    \item \textbf{A Novel Bi-Directional CoT Dataset} that teaches both forward (input-to-output) and backward (output-to-input) reasoning.
    \item \textbf{A Systematic Investigation of Data Utility} through targeted SFT experiments and evaluation to identify optimal training configurations.
    \item \textbf{An Open-Source Contribution} of our complete data synthesis pipeline to facilitate further research.
\end{enumerate}

The remainder of this paper is organized as follows. We first review related work in code reasoning and synthetic data generation. Next, we detail our three-stage data synthesis pipeline, followed by our experimental design and a thorough analysis of the results. We conclude with a discussion of our findings and future research directions.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.95\textwidth]{cot_diagram_v4.pdf}
    \caption{An overview of our three-stage data synthesis pipeline. 
    \textbf{Stage A (Concept Sourcing \& Synthesis):} Generates candidate concepts, code, and tests from raw documents. 
    \textbf{Stage B (Verification \& Clustering):} Uses our execution-based Dual Agreement algorithm to identify and rank the highest-quality solution-test pairs. 
    \textbf{Stage C (CoT Generation):} Uses the verified artifacts and their execution traces to produce the final, bi-directional conversational data, complete with trace-grounded rationales and feedback.}
    \label{fig:system_overview}
\end{figure*}

\begin{table*}[t!]
\centering
\caption{Comparison of features in related works on code reasoning data and methodologies. Our work is the first to systematically combine execution-grounded verification with bi-directional CoT generation at scale.}
\label{tab:related_work_comparison}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|ccccc}
\toprule
\textbf{Work / Method} & \textbf{Execution-Trace} & \textbf{Natural Language} & \textbf{Bi-Directional} & \textbf{Verifies} & \textbf{Scalable Data} \\ 
& \textbf{Grounded CoT} & \textbf{CoT} & \textbf{CoT} & \textbf{Output} & \textbf{Synthesis} \\
\midrule
REVTHINK~\cite{chen2024reverse} & \xmark & \cmark & \cmark & \xmark & \cmark \\
TRACED~\cite{ding2024traced} & \xmark & \xmark & \xmark & \cmark & \cmark \\
CodeI/O~\cite{li2025codeio} & \xmark & \cmark & \cmark & \cmark & \cmark \\
rStar-Coder~\cite{liu2025rstarcoderscalingcompetitivecode} & \xmark & \cmark & \xmark & \cmark & \cmark \\
Jung et al.~\cite{jung2025codeexecutiongroundedsupervision} & \cmark & \cmark & \xmark & \cmark & \xmark \\
SemCoder~\cite{ding2024semcodertrainingcodelanguage} & \xmark & \cmark & \cmark & \cmark & \cmark \\
\midrule 
\textbf{Our Work} & \textbf{\gcmark} & \textbf{\gcmark} & \textbf{\gcmark} & \textbf{\gcmark} & \textbf{\gcmark} \\
\bottomrule
\end{tabular}% 
}
\end{table*}




\section{Related Work}
Our work builds upon several interconnected research areas in code reasoning, execution-based program understanding, and synthetic data generation. Chain-of-Thought (CoT) prompting~\cite{wei2022chain} has proven effective for improving reasoning, but its extension to code faces unique verification challenges. While methods like Program-of-Thought (PoT)~\cite{chen2022program} generate executable snippets, they lack systematic verification of the reasoning process itself.
Building on this foundation, CodeI/O~\cite{li2025codeio} takes a significant step forward by transforming code patterns into natural language CoT rationales through input/output prediction tasks. By training models to predict inputs and outputs given code, CodeI/O exposes models to universal reasoning primitives while decoupling reasoning from syntax. However, CodeI/O focuses on prediction tasks rather than generating complete reasoning datasets from execution traces, which is the core contribution of our work.

A central theme in recent work is "verification," but the term is often used ambiguously. To clarify our
     contribution, we define a taxonomy of verification for code generation: 1) \textbf{Outcome Verification}, the most
     common approach, validates only the final output of a program against a test case. Benchmarks like HumanEval and
     methods like SemCoder use this, but it offers no guarantee about the logical integrity of the intermediate
     reasoning steps. 2) \textbf{Formal Verification}, which uses mathematical proofs to guarantee program correctness
     but is heavyweight and not typically used for LLM rationale generation. 3) \textbf{Rationale-Step Verification},
     the method we introduce. By generating our rationale directly from a program's operational semantics captured in 
     an execution trace, we can verify every step of the reasoning process itself. Our work is the first to 
     systematically generate CoT data with this level of soundness.
     
Recent work has increasingly recognized the value of program execution for creating training data. TRACED~\cite{ding2024traced} introduces execution-aware pre-training that incorporates dynamic program states, while Execution Tuning (E.T.)\citep{armengol2025execution} trains models on execution traces at different granularities. A highly relevant recent work, SemCoder~\cite{ding2024semcodertrainingcodelanguage}, also generates a synthetic dataset for fine-tuning. However, our methodology is fundamentally different in two key aspects. \textbf{First is the nature of the CoT verification.} Our pipeline performs a \textbf{direct translation of a program's execution trace} into a natural language rationale, making the reasoning process itself verifiable by construction. This contrasts with SemCoder, which relies on an \textbf{LLM generated CoT steps where only the final outcome is matched} against an execution result, leaving the intermediate reasoning steps unverified. \textbf{Second is the data generation philosophy.} Our pipeline follows a \textit{``concept-first''} approach, synthesizing problems from abstract concepts extracted from diverse sources like books and documentation. This provides fine-grained control over the complexity and diversity of the resulting data, a key advantage over the \textit{``code-first''} approach used by SemCoder, which is seeded by a fixed set of existing programs. Conceptually closest to our CoT generation step, Jung et al.\cite{jung2025codeexecutiongroundedsupervision} translate execution traces into natural language CoT. However, their work focuses on single-direction reasoning and does not systematically explore the bi-directional synthesis pipeline that we propose.

The principle of bi-directional reasoning, central to our work, has been explored in code by CodeI/O~\cite{li2025codeio} through I/O prediction tasks, and more broadly in other domains by works like FOBAR~\cite{jiang2024forward} and REVTHINK~\cite{chen2024reverse}. However, these prior approaches either focus on prediction tasks rather than generating complete rationales, or operate in domains where verification is less systematic than with code execution. Similarly, other verification techniques like Self-Verification~\cite{weng2023large} and RCoT~\cite{xue2023rcot} often rely on complex, potentially error-prone checking procedures, a limitation our deterministic, trace-based method avoids.

% The generation of high-quality synthetic training data remains a central challenge in code reasoning. Traditional approaches rely on Symbolic Knowledge Distillation (SKD)\cite{west2022symbolic,li2023symbolic}, where teacher models generate CoT rationales. A significant recent contribution is rStar-Coder\cite{liu2025rstarcoderscalingcompetitivecode}, which scales up verified data for competitive programming through an iterative self-improvement loop of generation and execution-based verification. While both our work and rStar-Coder leverage execution, our approaches differ in two fundamental ways. \textbf{First is the granularity of verification:} rStar-Coder verifies the \textit{final outcome} of the code. In contrast, our pipeline verifies \textit{every intermediate step of the reasoning process} by generating the rationale directly from a ground-truth execution trace. \textbf{Second is the source of the rationale:} The rationales in rStar-Coder's data are model-generated explanations. Our rationales, being direct narrations of a trace, are correct by construction, eliminating potential hallucinations. Therefore, while rStar-Coder excels at creating a large-scale dataset of verifiably correct \textit{solutions}, our work is the first to create a dataset of verifiably correct \textit{reasoning processes} in a systematic, bi-directional format.

The generation of high-quality synthetic data remains a central challenge. A significant recent contribution is rStar-Coder\cite{liu2025rstarcoderscalingcompetitivecode}, which scales up verified data for competitive programming. While both our work and rStar-Coder leverage execution, our approaches differ fundamentally in the \textbf{granularity of verification} and the \textbf{source of the rationale}. rStar-Coder verifies the \textit{final outcome} of the code, whereas our pipeline verifies \textit{every intermediate reasoning step} by generating the rationale directly from the trace. Consequently, our rationales are correct by construction, eliminating the potential for logical hallucinations present in LLM-generated explanations. Therefore, while rStar-Coder excels at creating a large-scale dataset of verifiably correct \textit{solutions}, our work is the first to create a dataset of verifiably correct \textit{reasoning processes} in a systematic, bi-directional format.

While existing work has explored execution traces for evaluation and limited training, significant gaps remain. As summarized in Table \ref{tab:related_work_comparison}, there is no systematic pipeline for generating complete, \textbf{bi-directional and CoT datasets where every reasoning step is grounded in verifiable execution}. Our work addresses this limitation, providing a trustworthy foundation for training robust code reasoning capabilities.



\section{Data Synthesis Pipeline}
Our data synthesis pipeline is a multi-stage process designed to generate high-fidelity, bi-directional CoT data where every reasoning step is anchored in ground-truth program execution. This ensures that the resulting dataset is correct by construction, eliminating the logical hallucinations common in purely model-generated rationales.

\subsection{Stage A: Concept Sourcing and Curriculum-Driven Synthesis}
Rather than generating code from simple prompts, our pipeline begins by building a curriculum of programming concepts derived from high-quality sources. This curriculum-driven approach ensures the resulting problems are complex, diverse, and grounded in established knowledge. The process unfolds in several automated steps:
\begin{enumerate}[nosep, leftmargin=*]
    \item \textbf{Automated Concept Extraction:} The pipeline begins by processing a corpus of permissively-licensed technical books, documentation (e.g., Python docs), and tutorials. For each text chunk, we employ a hybrid approach to identify core topics: standard NLP tools (\texttt{spaCy} with \texttt{pytextrank}) perform an initial keyword extraction, after which a teacher model refines this list—filtering out noise, completing partial phrases, and suggesting additional relevant concepts present in the text.

    \item \textbf{Quality-Based Concept Scoring:} Not all concepts are equally valuable for building advanced reasoning skills. To prioritize, we use a teacher model to score each extracted concept along two axes: \textbf{Difficulty} (1-5 scale) and \textbf{Relevance} (1-5 scale). The scoring prompt is engineered to favor concepts that can inspire complex algorithmic problems (e.g., involving dynamic programming or graph traversals).

    \item \textbf{Hierarchical Problem Synthesis:} For each high-scoring concept, the pipeline synthesizes a complete problem set through a hierarchical process. This begins by generating multiple natural language problem \textbf{Instructions}, then defines a formal code \textbf{Signature} for each. Using this as a scaffold, it generates multiple candidate \textbf{Solutions} and a comprehensive suite of \textbf{Unit Tests} to validate them.
\end{enumerate}
This structured generation process yields a rich set of interconnected artifacts—concept, instruction, solutions, and tests—that form the foundation for the subsequent stages.

\input{listings}

\subsection{Stage B: Execution-Based Verification and Agreement Clustering}
To move from a large pool of candidate solutions and tests to a high-confidence, verified set, we employ a sophisticated execution-based filtering stage adapted from the CodeT methodology~\cite{chen2022codetcodegenerationgenerated}. The process is as follows:
\begin{enumerate}[nosep, leftmargin=*]
    \item \textbf{Mass Execution:} For each problem (\texttt{task\_id}), every candidate solution generated in Stage A is executed against every corresponding candidate test case in a secure, sandboxed environment, creating a comprehensive pass/fail execution matrix.

    \item \textbf{Dual Agreement Clustering:} The core of this stage is the Dual Agreement algorithm. The insight is that while a single solution or test may be flawed, it is statistically unlikely that a large set of incorrect solutions would all be passed by a large set of incorrect tests. Solutions are partitioned into clusters where all solutions within a single cluster share the \textit{exact same} set of passed tests.

    \item \textbf{Cluster Scoring and Ranking:} Each cluster is assigned an agreement score to quantify its quality, calculated as $ \text{Score}(C) = |C| \times |T_p| $, where $|C|$ is the number of solutions in the cluster and $|T_p|$ is the number of tests they all pass. A high score signals strong consensus and correctness.

    \item \textbf{Best Case Selection:} The clusters for each \texttt{task\_id} are ranked by their score. We then select the solution(s) from the single, highest-scoring cluster and its corresponding verified test cases to serve as the trusted foundation for the next stage.
\end{enumerate}


% \subsection{Stage C: Execution-Grounded CoT Generation}
% The final stage transforms the verified solution-test pairs from Stage B into a rich, conversational dataset. This is not a single-prompt process, but a meticulous, automated pipeline that builds each component of the forward and backward reasoning chains.


% \begin{enumerate}
%     \item \textbf{Execution Tracing:} For each verified solution and its set of passing test cases, we first generate a ground-truth execution trace. We use \texttt{pysnooper} to instrument the specific function or method identified in the problem's signature information. The solution is then executed against each individual test case in a secure, sandboxed process with a strict timeout. This produces a raw, line-by-line trace file detailing every variable assignment and state change for each successful execution.

%     \item \textbf{Trace Sanitization:} The raw \texttt{pysnooper} logs contain formatting artifacts and ANSI color codes not suitable for model consumption. A dedicated script cleans each trace file, removing these artifacts to produce a clean, plain-text representation of the program's execution flow.

%     \item \textbf{Bi-Directional Conversation Synthesis:} This is the core of the stage, where the cleaned trace is used to generate the final conversational data. For each test case, the pipeline performs the following sequence of LLM-driven steps:
%     \begin{itemize}
%         \item \textbf{Ground-Truth I/O Extraction.} The pipeline first establishes the precise input and output for the test case. 
%         % It intelligently categorizes tests: for simple, standalone \texttt{assert} statements, it uses regular expressions to directly parse the function call input and the expected output. For more complex tests involving setup code, it uses a targeted LLM prompt to analyze the test and extract the effective input/output pair.
        
%         \item \textbf{Question Generation.} With the ground-truth input and output established, an LLM is prompted to generate a pair of natural language questions: a \textbf{forward question} (e.g., "Given the input `[1, 2]`, what does the function return?") and a corresponding \textbf{backward question} (e.g., "What input would cause the function to return `3`?").
        
%         \item \textbf{Trace-Grounded CoT Generation.} The cleaned execution trace is now used as the factual basis for generating the reasoning chains.
%         \begin{itemize}
%             \item For the \textit{forward question}, an LLM is prompted to narrate the execution trace, explaining step-by-step how the given input is transformed into the final output. This narrative forms the forward Chain-of-Thought. The prompt explicitly instructs the model to extract the \texttt{<Predicted Output>} it deduces from the trace.
%             \item For the \textit{backward question}, a separate LLM prompt asks the model to perform deductive reasoning. Using the trace as evidence, it must explain how the final output state could only have been reached from the initial input. This forms the backward Chain-of-Thought, and the model is asked to extract the \texttt{<Predicted Input>} it derives. Refer~\ref{fig:cot-templates}.
%         \end{itemize}
        
%         % \item \textbf{Verifiable Feedback Generation.} The pipeline creates a final layer of verification. The `predicted\_output` from the forward CoT is compared to the ground-truth output, and an LLM generates a \textbf{feedback response} confirming the match and explaining why it is correct based on the problem. The same process is repeated for the `predicted\_input` from the backward CoT.
%     \end{itemize}

%     \item \textbf{Final Assembly:} All the generated components—the questions, the trace-grounded CoTs, the predicted I/O-are assembled into conversational format. 
%     % The final output is a JSONL file where each line represents a complete, bi-directional reasoning conversation, grounded in verifiable program execution.
% \end{enumerate}


\subsection{Stage C: Execution-Grounded CoT Generation}
The final stage transforms the verified solution-test pairs into a rich, conversational dataset through a meticulous, automated pipeline.
\begin{enumerate}[nosep, leftmargin=*]
    \item \textbf{Trace Generation and Sanitization:} For each verified solution-test pair, we generate a detailed execution trace using \texttt{pysnooper}. This raw log, which captures all variable state changes, is then sanitized to produce a clean, model-ingestible text representation of the execution flow.

    \item \textbf{Bi-Directional CoT Synthesis:} The core of this stage is a multi-step LLM workflow. For each test case, we first establish the ground-truth input/output pair and generate corresponding forward and backward questions. The sanitized trace then serves as the factual basis for generating two distinct rationales: an LLM narrates the trace to create a \textbf{forward CoT} explaining how the input leads to the output, and another prompt guides the model to use the same trace for a deductive \textbf{backward CoT}. These components are then assembled into final, structured formats; for example, a forward sample follows the precise sequence of \texttt{<Instruction>}, \texttt{<Function>}, \texttt{<Forward Question>}, \texttt{<CoT>}, and \texttt{<Predicted Output>}, whereas a backward sample follows the sequence \texttt{<Instruction>}, \texttt{<Function>}, \texttt{<Backward Question>}, \texttt{<CoT>}, and \texttt{<Predicted Input>}.
\end{enumerate}

\section{Experimental Design}
Our experiments are designed to rigorously quantify the benefits of our execution-grounded data and to determine the optimal configurations for fine-tuning. 
% We adopt a systematic, top-down ablation approach, first identifying the most effective data curation strategy and then the best reasoning format. This entire process is conducted on two distinct model families to evaluate both efficacy and generalizability.

% \subsection{Models and Benchmarks}
% Our experiments use two open-source base models of a similar scale: \texttt{granite-3.3-8b-base}~\cite{granite3.3-base} as our primary model and \texttt{Qwen2.5-Coder-7B}~\cite{hui2024qwen2} to validate the generalization of our approach on a distinct, coding-focused architecture. We evaluate all models on LiveCodeBench (Execution)~\cite{jain2024livecodebench} and CruxEval (Input/Output)~\cite{gu2024cruxeval} benchmarks specifically chosen for their focus on code execution reasoning, such as predicting outputs from inputs and, in the case of CruxEval, vice versa.

\subsection{Models and Benchmarks}
Our model selection was deliberately designed to test our method across two open-source foundation models of a similar scale but with distinct pre-training philosophies. We selected \texttt{granite-3.3-8b-base}~\cite{granite3.3-base}, an enterprise-grade architecture trained exclusively on permissively licensed data, which validates our approach for real-world applications where data provenance is critical. Alongside it, we chose \texttt{Qwen2.5-Coder-7B}~\cite{hui2024qwen2}, a state-of-the-art, code-native specialist, to test if our data provides a reasoning signal capable of enhancing even expert models. The ability to improve both a safe generalist and a top specialist demonstrates the fundamental and broad applicability of our method. We evaluate all models on LiveCodeBench (Execution)~\cite{jain2024livecodebench} and CruxEval (Input/Output)~\cite{gu2024cruxeval}, benchmarks specifically chosen for their focus on code execution reasoning. We finetune the base models for 10 epochs with a starting LR of 2e-6. More details about the training setup are included in the Appendix.

\subsection{Data Curation and Preparation}
The foundation of our experiments is a large-scale, high-quality dataset generated by our synthesis pipeline.
\begin{enumerate}
    \item \textbf{Concept Sourcing and Scaling:} The pipeline began by ingesting a diverse set of permissively-licensed documents, including those from the "Free Programming Books" source in the StarCoder2-documentation dataset~\cite{starcoder2-doc} and curated lists of programming books. These sources cover a wide range of topics, from core Python language features and data structures to advanced algorithms. This process yielded an initial set of approximately 15,000 deduplicated concepts. From this seed, we hierarchically synthesized a large pool of programming problems: for each concept, we generated multiple distinct instructions, and for each instruction, multiple candidate solutions and unit tests.

    \item \textbf{Verification and Deduplication:} This large, raw set of candidates was passed through our execution-based verification stage (Stage B), resulting in approximately 85,000 code-test pairs that were confirmed to be functionally correct. A final deduplication pass on similar code snippets and test logic yielded our initial master dataset.

    \textbf{CoT Formatting:} Each sample in the master set was formatted into three distinct reasoning structures. A \textbf{forward CoT} sample presents a question like \texttt{Given input X, what is the output?} followed by a trace-narrated rationale. A \textbf{backward CoT} sample asks \texttt{"What input could produce output Y?"} followed by a deductive, trace-grounded rationale, as shown in Figure~\ref{fig:cot-templates}. Finally, a \textbf{bi-directional} sample combines these into a single, multi-turn conversational format.

    \item \textbf{Difficulty-Based Subsetting:} From the master dataset, we created three distinct subsets for our ablation studies, based on different filtering strategies:
    \begin{itemize}
        \item \textbf{Full Set (54k):} This is the complete, verified, and deduplicated dataset, representing the broadest collection of concepts.
        \item \textbf{Model-Derived Difficulty Set (25k):} This high-quality subset was created via a novel filtering strategy: we kept only the problems from the 54k set that an LLM failed to solve correctly. This creates a targeted curriculum focused on the model's specific weaknesses.
        \item \textbf{Content-Rated Difficulty Set (18k):} This set was further refined from the 25k set by using an LLM to rate the conceptual difficulty of each problem, retaining only those rated as "medium" or "hard".
    \end{itemize}
\end{enumerate}
This curation process provides two key dimensions for our experiments: the data curation strategy (54k vs. 25k vs. 18k) and the reasoning direction (forward, backward, or bi-directional).

  


% Define a new column type for centered, wrapping text. Place this right before your table.
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}

\begin{table*}[t!]
    \centering
    \caption{Comprehensive evaluation results across models, datasets, and training configurations on CruxEval and LiveCodeBench-Exec benchmarks. The experiments follow a top-down approach, with the winning configuration
    from one stage used in the next. Best results for each stage are highlighted in \textbf{bold}. And best result across the exp. stages are highlighted in \textbf{grey}.}
    \label{tab:comprehensive-results}
    \resizebox{\textwidth}{!}{%
    % Use the new C column type for the first two columns. Adjust the width as needed.
    \begin{tabular}{C{2.5cm}|C{2.5cm}|l|c|c|c|cc|cc}
    \hline
    \multirow{2}{*}{\textbf{Experiment Stage}} & \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Training Config}} & \multirow{2}{*}{\textbf{Data Subset}} & \multirow{2}{*}{\textbf{Data Config}} &
    \textbf{LiveCodeBench-Exec} &
    \multicolumn{2}{c|}{\textbf{CruxEval Output}} & \multicolumn{2}{c}{\textbf{CruxEval Input}} \\
     & & & & & \textbf{Pass@1} & \textbf{Pass@1} & \textbf{Pass@5} & \textbf{Pass@1} & \textbf{Pass@5} \\
         \hline
    \hline
    % \multicolumn{9}{c}{\textit{Granite-3.3-8B Model Family}} \\
    % \hline
    \parbox{2.5cm}{\centering Baselines} & \parbox{2.5cm}{\centering Granite-3.3-8B-base} & Base (default) & N/A & N/A & 18.3 & 15.5 & 25.3 & 14.3 & 28.4 \\
    \hline
    \multirow{3}{*}{\parbox{2.5cm}{\centering Data Curation}} & \multirow{6}{*}{\parbox{2.5cm}{\centering Granite-3.3-8B (FT)}} & \multirow{3}{*}{Fwd} & 18k & Difficulty-Filt. & 43.5$_{\textcolor{blue}{(+25.2)}}$ & 36.1$_{\textcolor{blue}{(+20.6)}}$ & 58.2$_{\textcolor{blue}{(+32.9)}}$ & 35.8$_{\textcolor{blue}{(+21.5)}}$ & 57.9$_{\textcolor{blue}{(+29.5)}}$ \\
     & & & 25k (best perf.) & Correctness-Filt. & \textbf{44.9}$_{\textcolor{blue}{(+26.6)}}$ & \textbf{42.7}$_{\textcolor{blue}{(+27.2)}}$ & \textbf{64.7}$_{\textcolor{blue}{(+39.4)}}$ & \textbf{40.2}$_{\textcolor{blue}{(+25.9)}}$ &
    \textbf{63.5}$_{\textcolor{blue}{(+35.1)}}$ \\
     & & & 54k & Full Set & 34.1$_{\textcolor{blue}{(+15.8)}}$ & 28.9$_{\textcolor{blue}{(+13.4)}}$ & 55.2$_{\textcolor{blue}{(+29.9)}}$ & 28.8$_{\textcolor{blue}{(+14.5)}}$ & 54.9$_{\textcolor{blue}{(+26.5)}}$ \\
    \cline{1-1}\cline{3-10}
    \multirow{3}{*}{\parbox{2.5cm}{\centering Reasoning Direction}} & & Fwd & 25k & Correctness-Filt. & \cellcolor{highlightgray}{\textbf{44.9}$_{\textcolor{blue}{\textbf{(+26.6)}}}$} & 42.7$_{\textcolor{blue}{(+27.2)}}$ & 64.7$_{\textcolor{blue}{(+39.4)}}$ & 40.2$_{\textcolor{blue}{(+25.9)}}$ & 63.5$_{\textcolor{blue}{(+35.1)}}$ \\
     & & Bwd & 25k & Correctness-Filt. & 35.4$_{\textcolor{blue}{(+17.1)}}$ & 39.3$_{\textcolor{blue}{(+23.8)}}$ & 61.3$_{\textcolor{blue}{(+36.0)}}$ & 41.5$_{\textcolor{blue}{(+27.2)}}$ & 64.8$_{\textcolor{blue}{(+36.4)}}$ \\
     & & Bi-directional & 25k & Correctness-Filt. & 44.3$_{\textcolor{blue}{(+26.0)}}$ & \cellcolor{highlightgray}{\textbf{45.7}$_{\textcolor{blue}{\textbf{(+30.2)}}}$} & \cellcolor{highlightgray}{\textbf{67.4}$_{\textcolor{blue}{\textbf{(+42.1)}}}$} & \cellcolor{highlightgray}{\textbf{42.1}$_{\textcolor{blue}{\textbf{(+27.8)}}}$} & \cellcolor{highlightgray}{\textbf{65.2}$_{\textcolor{blue}{\textbf{(+36.8)}}}$} \\
    \hline
    \hline
    % \multicolumn{9}{c}{\textit{Qwen2.5-7B Model Family}} \\
    % \hline
    \parbox{2.5cm}{\centering Baselines} & \parbox{2.5cm}{\centering Qwen2.5-Coder-7B} & Base (default) & N/A & N/A & 46.3 & 45.3 & 52.12 & 47.5 & 49 \\
    \hline
    \multirow{3}{*}{\parbox{2.5cm}{\centering Data Curation}} & \multirow{6}{*}{\parbox{2.5cm}{\centering Qwen2.5-Coder-7B (FT)}} & \multirow{3}{*}{Fwd} & 18k & Difficulty-Filt. & 66.9$_{\textcolor{blue}{(+20.6)}}$ & 58.4$_{\textcolor{blue}{(+13.1)}}$ & 75.5$_{\textcolor{blue}{(+23.4)}}$ &
    57.2$_{\textcolor{blue}{(+9.7)}}$ & 69.4$_{\textcolor{blue}{(+20.4)}}$ \\
     & & & 25k & Correctness-Filt. & \textbf{67.0}$_{\textcolor{blue}{(+20.7)}}$ & 57.5$_{\textcolor{blue}{(+12.2)}}$ & 73.9$_{\textcolor{blue}{(+21.8)}}$ & 59.8$_{\textcolor{blue}{(+12.3)}}$ & 67.5$_{\textcolor{blue}{(+18.5)}}$ \\
     & & & 54k (best perf.)& Full Set & 66.5$_{\textcolor{blue}{(+20.2)}}$ & \textbf{58.6}$_{\textcolor{blue}{(+13.3)}}$ & \textbf{76.0}$_{\textcolor{blue}{(+23.9)}}$ & \textbf{60.5}$_{\textcolor{blue}{(+13.0)}}$ & \textbf{68.3}$_{\textcolor{blue}{(+19.3)}}$ \\
    \cline{1-1}\cline{3-10}
    \multirow{3}{*}{\parbox{2.5cm}{\centering Reasoning Direction}} & & Fwd & 25k & Correctness-Filt. & 67.0$_{\textcolor{blue}{(+20.7)}}$ & 57.5$_{\textcolor{blue}{(+12.2)}}$ & 73.9$_{\textcolor{blue}{(+21.8)}}$ & 59.8$_{\textcolor{blue}{(+12.3)}}$ & 67.5$_{\textcolor{blue}{(+18.5)}}$ \\
     & & Bwd & 25k & Correctness-Filt. & 57.5$_{\textcolor{blue}{(+11.2)}}$ & 50.4$_{\textcolor{blue}{(+5.1)}}$ & 69.8$_{\textcolor{blue}{(+17.7)}}$ & 61.2$_{\textcolor{blue}{(+13.7)}}$ & 69.1$_{\textcolor{blue}{(+20.1)}}$ \\
     & & Bi-directional & 25k & Correctness-Filt. & \cellcolor{highlightgray}{\textbf{68.2}$_{\textcolor{blue}{\textbf{(+21.9)}}}$} & \cellcolor{highlightgray}{\textbf{59.7}$_{\textcolor{blue}{\textbf{(+14.4)}}}$} & \cellcolor{highlightgray}{\textbf{75.4}$_{\textcolor{blue}{\textbf{(+23.3)}}}$} & \cellcolor{highlightgray}{\textbf{61.9}$_{\textcolor{blue}{\textbf{(+14.4)}}}$} & \cellcolor{highlightgray}{\textbf{70.2}$_{\textcolor{blue}{\textbf{(+21.2)}}}$} \\

    \hline
    \end{tabular}%
    }
    \footnotesize
      \textit{Note:} All Pass@k scores are reported as percentages. Green subscript values show improvement over the Base (Pre-trained) baseline. Abbreviations: Filt. = Filtered, perf. = performing, Fwd: Forward only CoT samples, Bwd: Backward only CoT samples, FT: Supervised fine-tuned.

    \end{table*}

% \subsection{Ablation Study Plan}
% Our experiments are designed as a sequential, top-down filter to find the best data configuration. All experiments are conducted in a Supervised Fine-Tuning (SFT) setting.
% \begin{enumerate}
%     \item \textbf{Data Curation Ablation:} We first aim to identify the optimal data subset. We fine-tune each base model on the 54k, 25k, and 18k datasets using only the forward CoT samples and compare their performance.
%     \item \textbf{Reasoning Direction Ablation:} Using the winning data subset from the previous stage, we then investigate the impact of reasoning format. We fine-tune models on forward-only, backward-only, and the full bi-directional versions of the dataset. \textcolor{blue}{We only used the winning subset considering the fact that backward-only version only replaces forward question-response (q-r) pair with backward q-r pair in each sample, similarly bi-directional version augments each sample by adding backward q-r pair. The input code and test case used for generating questions in each sample remain the same across all the three versions of the data subset (forward, backward and bi-directional). This also helped to reduce the compute requirements by avoiding additional trainings.}
%     \item \textbf{Model Generalization:} By performing this two-stage process on both Granite and Qwen-Coder, we validate the generalization of our findings across different model architectures.
% \end{enumerate}


\subsection{Ablation Study Plan}
Our experiments are designed as a sequential, top-down filter to efficiently identify the optimal data configuration. While testing every combination of our data subsets and reasoning formats would be ideal, such a full factorial experiment is computationally infeasible. Our sequential approach is a practical and methodologically sound alternative that allows us to isolate the impact of our two primary contributions: difficulty-filtered subsets and CoT directionality. We operate on the reasonable assumption that the data subset demonstrating the highest quality for forward CoT will also be the most effective foundation for the other CoT formats. All experiments are conducted in a Supervised Fine-Tuning (SFT) setting.

\begin{enumerate}
    \item \textbf{Data Curation Ablation:} We first aim to identify the optimal data subset. We fine-tune each base model on the 54k, 25k, and 18k datasets using only the forward CoT samples and compare their performance.
    \item \textbf{CoT Direction Ablation:} Using the winning data subset from the previous stage, we then investigate the impact of the CoT format by fine-tuning models on the forward-only, backward-only, and the full bi-directional versions of that dataset.
    \item \textbf{Model Generalization:} By performing this two-stage process on both Granite and Qwen-Coder, we validate the generalization of our findings across different model architectures.
\end{enumerate}





\section{Results and Analysis}
We conduct a series of systematic ablation studies to evaluate the effectiveness of our data synthesis pipeline and determine the optimal training configurations. Our experimental design follows a sequential, top-down filtering approach: we first identify the best-performing data curation strategy, then use that dataset to determine the most effective reasoning direction. This entire process is performed on \texttt{Granite-3.3-8b-base}, and then repeated for \texttt{Qwen2.5-Coder-7B} to validate the generalizability of our findings. We evaluate all models on LiveCodeBench (Execution) and CruxEval (Output and Input prediction).

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\columnwidth]{instruct_boost_single.png}
    \caption{Performance boost from fine-tuning instruct models. Solid bars represent the baseline instruct models; hatched bars show the improvement after fine-tuning with the best-performing 25k bi-directional subset of data.}
    \label{fig:instruct-boost}
    \vspace{-10pt}
\end{figure}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\columnwidth]{sota_comparison_final.png}
    \caption{Performance comparison against SOTA baselines~\cite{ding2024semcodertrainingcodelanguage} Our model's results are highlighted with a hatch pattern. \textit{Abbreviations:} CL: CodeLlama, SC2: StarCoder2, DSCoder: DeepSeekCoder, MCoder: MagicCoder, -Py: Python, -Inst: Instruct.}
    \label{fig:sota-comparison}
    \vspace{-12pt}
\end{figure}
The comprehensive results of these experiments are presented in Table \ref{tab:comprehensive-results}.

\subsection{Impact of Data Curation}
Our first primary ablation investigates the impact of data quality and size by fine-tuning models on three forward-only data subsets: a complete 54k sample set, a higher-quality 25k set filtered for correctness, and a challenging 18k set filtered for difficulty.

For the \texttt{Granite-3.3-8b} model, the results are definitive. As shown in Table \ref{tab:comprehensive-results}, the \textbf{25k correctness-filtered dataset} substantially outperforms the others across all benchmarks. On LiveCodeBench, it achieves a score of 44.9\%, an absolute gain of \textbf{+26.6} over the base model, and significantly outperforms the larger 54k set (34.1\%). This powerful improvement from a targeted, high-quality dataset strongly indicates that for complex reasoning, data verifiability is far more impactful than sheer volume.

When repeating the experiment with \texttt{Qwen2.5-Coder-7B}, all three fine-tuned models dramatically outperform the base model. For instance, the 25k dataset boosts the LiveCodeBench score from 46.3\% to 67.0\% (\textbf{+20.7}). Unlike with Granite, the performance across the three data subsets is highly competitive. The 25k dataset achieves the highest score on LiveCodeBench, while the 54k dataset has a slight edge on the CruxEval benchmarks. Given the consistently strong performance of the 25k set, we selected it as the winning configuration for all subsequent experiments.



\subsection{Impact of Reasoning Direction}
Using the winning 25k dataset from the previous stage, we evaluated the impact of our novel bi-directional data format. We compared models trained on forward-only, backward-only, and the complete bi-directional datasets.

For the \texttt{Granite-3.3-8b} model, the results in Table \ref{tab:comprehensive-results} highlight the benefits of bi-directional training. The model trained on the \textbf{bi-directional dataset} achieves the highest scores on both CruxEval Output (45.7\% pass@1) and Input (42.1\% pass@1). This represents a final performance gain of \textbf{+30.2} on CruxEval Output over the base model, demonstrating that teaching cause-to-effect and effect-to-cause reasoning provides a synergistic improvement.

This finding is further confirmed with the \texttt{Qwen2.5-Coder-7B} model. The bi-directionally trained model once again emerges as the top performer, achieving the best results on LiveCodeBench (68.2\%) and CruxEval Input (61.9\% pass@1). This configuration's final score on LiveCodeBench represents a \textbf{+21.9} point gain over the base model. To contextualize this performance, Figure \ref{fig:sota-comparison} compares our best model against a range of competitive foundation models. Our fine-tuned model establishes a leading score on LiveCodeBench-Execution and demonstrates highly competitive performance across all reasoning benchmarks.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.7\linewidth]{clean_best_plots.png}
    \caption[CoT quality analysis]{Chain-of-Thought quality analysis. Top: CoT-to-outcome consistency vs reasoning length with regression analysis. Bottom: Information content distributions. Verified CoT training shows superior consistency scaling ($R^2 = 0.122$ vs $0.011$) and 761\% higher information richness ($d = 7.93$, $p < 0.001$).}

    \label{fig:cot-quality}
\end{figure}

\subsection{Enhancing Instruction-Tuned Models}
To confirm that our best-performing bi-directional data provides a specialized signal complementary to general instruction tuning, we fine-tuned existing instruct-tuned models. As shown in Figure \ref{fig:instruct-boost}, this provides a substantial boost, yielding a massive \textbf{+39.9} gain on CruxEval Input for \texttt{granite-3.3-8B-instruct} and a \textbf{+21.5} gain on CruxEval Output for the already strong \texttt{Qwen2.5-Coder-7B-Instruct}, demonstrating the value of our execution-grounded data.





\subsection{Chain-of-Thought Consistency and Quality Analysis}

  To assess the quality of reasoning generated by our fine-tuned model, we compare CoTs generated by our \texttt{Qwen2.5-Coder-7B-FT} verified CoT model against the base \texttt{Qwen2.5-Coder-7B} model on CruxEval-O problems. For each problem, we generate CoT from both models and evaluate two key dimensions: \textbf{consistency} between reasoning and final answer, and
  \textbf{information content} of the reasoning content.

  \textbf{Consistency Analysis.} We measure consistency using a composite metric that evaluates reasoning-answer alignment across multiple dimensions: entailment patterns, conceptual
  overlap, and sequential coherence. Figure~\ref{fig:cot-quality} (top) shows our model exhibits strong correlation between CoT length and consistency ($R^2 = 0.122$), indicating longer reasoning is more logically coherent. The baseline model shows no such relationship ($R^2 = 0.011$). Moreover, when CoT is consistent with the final answer, our model is more likely to produce correct solutions (AUC = 0.567) compared to the baseline where consistency poorly predicts correctness (AUC = 0.502).

  \textbf{Information Richness.} The bottom plot reveals that our model produces substantially more informative reasoning. Using an entropy-based metric that accounts for vocabulary diversity and technical term density, we find the CoTs from our fine-tuned model are \textbf{761\% more information-rich} than those from the baseline. This difference is statistically significant with a very large effect size (Cohen's $d = 7.93$, $p < 0.001$), confirming that our training method generates more detailed and semantically meaningful rationales.
  
  These results confirm that verified CoT training fundamentally enhances reasoning quality, producing more consistent and informative chain-of-thought processes that better support
  final outputs.

% \subsection{Chain-of-Thought Quality Analysis}
% To validate that our data improves not just accuracy but the reasoning process itself, we analyze the quality of the CoTs generated by our fine-tuned Qwen2.5 model versus those from the baseline Qwen2.5 model for problems in the CruxEval-O benchmark. We compare the CoTs along two axes: their internal consistency with the final answer and their information richness.

% \textbf{Consistency Analysis.} As shown in the top plot of Figure~\ref{fig, we find that the reasoning from our fine-tuned model is far more reliable. We use a composite score that measures consistency from multiple dimensions, including \textbf{entailment} (logical connectors near the answer), \textbf{conceptual alignment} (concept overlap), and \textbf{sequential coherence}. Our fine-tuned model's CoTs show a clear positive correlation between length and consistency ($R^2 = 0.122$), indicating that longer rationales build logically toward the conclusion. The baseline model's CoTs show no such relationship ($R^2 = 0.011$). Furthermore, the consistency score of our model's CoT is a much better predictor of a correct final answer, achieving a superior Area Under the ROC Curve (AUC) of 0.567 compared to the baseline's near-random 0.502.

% \textbf{Information Richness.} The bottom plot of Figure~\ref{fig:cot-quality} reveals that our model produces substantially more informative reasoning. Using an entropy-based metric that accounts for vocabulary diversity and technical term density, we find that the CoTs from our fine-tuned model are \textbf{761\% more information-rich} than those from the baseline. This difference is statistically significant with a very large effect size (Cohen's $d = 7.93$, $p < 0.001$), confirming that our training method generates more detailed and semantically meaningful rationales.
  
% These results confirm that verified CoT training fundamentally enhances reasoning quality, producing more consistent and informative chain-of-thought processes that better support
%   final outputs.  
%   % These findings demonstrate that verified CoT training not only improves accuracy but fundamentally enhances the quality of reasoning by producing more consistent, information-rich, and logically coherent chain-of-thought processes that better support the final model outputs through explicit logical connectors, conceptual alignment, and sequential coherence.



% \FloatBarrier
\section{Conclusion and Future Work}
We introduced a methodology for generating verifiable CoT data for code reasoning, addressing the challenge of logical integrity in synthetic datasets. By grounding every reasoning step in program execution traces, our pipeline eliminates logical hallucinations and produces high-fidelity training data that is correct by construction.

Our systematic evaluation confirms the effectiveness of this approach. We show that verified data quality is more important than quantity and that our novel bi-directional format significantly improves reasoning. These findings hold across different model architectures, where fine-tuning boosted performance by as much as \textbf{+30.2 points} on the CruxEval-O reasoning benchmark, underscoring the fundamental impact of our contribution.

Future work includes extending our language-agnostic pipeline to other languages like C++ and Java and exploring its use in advanced training schemes such as offline reinforcement learning with DPO. To aid the development of more robust language models, we will publicly release our complete synthesis pipeline.

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


%%
%% If your work has an appendix, this is the place to put it.
\appendix



\end{document}
\endinput
%%
%% End of file `sample-acmsmall-conf.tex'.

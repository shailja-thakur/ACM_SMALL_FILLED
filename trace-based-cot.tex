%%
%% This is file `sample-acmsmall-conf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,acmsmall-conf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-acmsmall-conf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[acmsmall,screen,review,anonymous,nonacm]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation email}{June 03--05,
  2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/2018/06}

\usepackage{tabularx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{amsmath}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
\usepackage{array}
\usepackage{subcaption}
\usepackage{natbib}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage[most]{tcolorbox}
\frenchspacing
\let\Bbbk\relax
\usepackage{bibentry}
\usepackage{amssymb}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{pifont}
\usepackage[dvipsnames]{xcolor}
\usepackage[table]{xcolor}
\definecolor{highlightgray}{gray}{0.85}
\definecolor{tracegray}{RGB}{100,100,100}
\definecolor{rationaleblue}{RGB}{0,102,204}
\definecolor{errorred}{RGB}{220,53,69}
\definecolor{correctgreen}{RGB}{40,167,69}
\usepackage{placeins}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage{algorithm}
\usepackage{algorithmic}
\newcommand{\gcmark}{\textcolor{ForestGreen}{\cmark}}%
\usepackage{newfloat}
\usepackage{listings}

%% Python syntax highlighting configuration
\lstdefinestyle{pythoncode}{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue}\bfseries,
    stringstyle=\color{red!70!black},
    commentstyle=\color{green!60!black}\itshape,
    numberstyle=\tiny\color{gray},
    showstringspaces=false,
    breaklines=true,
    frame=none,
    captionpos=b,
    tabsize=4,
    morekeywords={self,True,False,None},
}

\lstdefinestyle{tracestyle}{
    basicstyle=\ttfamily\scriptsize,
    backgroundcolor=\color{gray!10},
    frame=none,
    showstringspaces=false,
    breaklines=true,
}

%% Command for highlighting key technical terms
\newcommand{\techterm}[1]{\textcolor{rationaleblue}{\textbf{#1}}}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Let the Code Speak: Generating Verifiable CoT from Execution-Traces}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Ben Trovato}
\authornote{Both authors contributed equally to this research.}
\email{trovato@corporation.com}
\orcid{1234-5678-9012}
\author{G.K.M. Tobin}
\authornotemark[1]
\email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{Institute for Clarity in Documentation}
  \city{Dublin}
  \state{Ohio}
  \country{USA}
}

\author{Lars Th{\o}rv{\"a}ld}
\affiliation{%
  \institution{The Th{\o}rv{\"a}ld Group}
  \city{Hekla}
  \country{Iceland}}
\email{larst@affiliation.org}

\author{Valerie B\'eranger}
\affiliation{%
  \institution{Inria Paris-Rocquencourt}
  \city{Rocquencourt}
  \country{France}
}

\author{Aparna Patel}
\affiliation{%
 \institution{Rajiv Gandhi University}
 \city{Doimukh}
 \state{Arunachal Pradesh}
 \country{India}}

\author{Huifen Chan}
\affiliation{%
  \institution{Tsinghua University}
  \city{Haidian Qu}
  \state{Beijing Shi}
  \country{China}}

\author{Charles Palmer}
\affiliation{%
  \institution{Palmer Research Laboratories}
  \city{San Antonio}
  \state{Texas}
  \country{USA}}
\email{cpalmer@prl.com}

\author{John Smith}
\affiliation{%
  \institution{The Th{\o}rv{\"a}ld Group}
  \city{Hekla}
  \country{Iceland}}
\email{jsmith@affiliation.org}

\author{Julius P. Kumquat}
\affiliation{%
  \institution{The Kumquat Consortium}
  \city{New York}
  \country{USA}}
\email{jpkumquat@consortium.net}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  Large language models now power conversational code assistants, but struggle with multi-step reasoning tasks like debugging and program comprehension. Chain-of-Thought (CoT) fine-tuning—where models learn from step-by-step reasoning traces—has emerged as a solution, but faces a critical challenge: the inability to verify the logical integrity of reasoning steps, leading to models trained on plausible but incorrect "hallucinated" logic. Existing methods validate only final program outputs, leaving intermediate reasoning steps unverified and unsound. We introduce the first system to achieve \textbf{rationale-step verification} at scale by generating CoT rationales directly from program execution traces, making them \textbf{correct by construction}. Each reasoning step is verifiable: variable values match recorded runtime states, state transitions reflect actual semantic operations, and control flow describes executed branches. This approach eliminates logical hallucinations that plague LLM-generated explanations. We systematically create 54,000 high-fidelity, bi-directional rationales that teach models to reason both forward (input→output) and backward (output→input), a capability essential for debugging and program comprehension. Models fine-tuned on our verified data achieve substantial improvements: \textbf{+14.4 points} on CruxEval-Input and \textbf{+21.9 points} on LiveCodeBench-Exec, demonstrating that verification quality directly determines reasoning capabilities. The complete synthesis pipeline will be open-sourced.
\end{abstract>

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10011007.10011006.10011008</concept_id>
  <concept_desc>Software and its engineering~General programming languages</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010147.10010178.10010179</concept_id>
  <concept_desc>Computing methodologies~Machine learning</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10011007.10011074.10011099</concept_id>
  <concept_desc>Software and its engineering~Software verification and validation</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~General programming languages}
\ccsdesc[500]{Computing methodologies~Machine learning}
\ccsdesc[300]{Software and its engineering~Software verification and validation}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Chain-of-Thought, Code Reasoning, Execution Traces, Verification, Synthetic Data Generation, Program Semantics, Large Language Models}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
Recent advances in large language models have enabled conversational code assistants that can generate, explain, and debug programs through natural language interaction. These systems—exemplified by GitHub Copilot, ChatGPT for code, and similar tools—have demonstrated remarkable syntactic fluency. However, they struggle with tasks requiring deep reasoning about program behavior: debugging requires understanding how state evolves, program comprehension demands tracing execution paths, and code refactoring needs verifying semantic preservation. These reasoning tasks are fundamental to software engineering practice, yet current models often produce plausible but incorrect explanations of program behavior.

From a programming languages perspective, this reveals a deeper challenge: while LLMs are powerful static models of source code, they fundamentally lack a connection to the \textit{dynamic behavior} of programs—how state evolves during execution. This gap limits their utility to that of sophisticated auto-completes rather than true reasoning partners for tasks like debugging and program analysis, which require a faithful understanding of a program's operational semantics.

% To improve these reasoning capabilities, the research community has widely embraced fine-tuning models on Chain-of-Thought (CoT) data~\cite{wei2022chain}, which provides explicit, step-by-step rationales. However, sourcing this data for the domain of code is particularly challenging. Recent self-instruct methodologies use a powerful "teacher" model to generate massive datasets of code and explanations, but these suffer from a fundamental flaw: \textbf{the generated reasoning steps are not tethered to any ground truth}. The teacher model's explanation is often a plausible "hallucination" of the code's logic~\cite{turpin2023languagemodelsdontsay,beger2025coconutstructuralcodeunderstanding} rather than a factual account of its execution, leading to models being trained on subtly incorrect rationales.

To bridge this gap, the research community has embraced fine-tuning models on Chain-of-Thought (CoT) data, which provides explicit, step-by-step rationales to guide the model's reasoning process. However, the methods used to generate this data for code are often \textit{unsound}. A powerful "teacher" model generates the rationale, but these reasoning steps are not tethered to a program's actual execution trace. The result is a dataset of "logical hallucinations"—plausible but factually incorrect accounts of the program's logic.

This challenge is especially acute in code reasoning, where small logical errors cascade into flawed understanding. Consider a model trained on a rationale claiming "variable \texttt{count} is incremented in the loop"—when in fact, \texttt{count} is decremented. Such errors propagate during training, teaching the model to follow plausible but incorrect logical patterns. This lack of guaranteed soundness is a critical barrier for high-stakes applications. For \textbf{debugging}, a rationale that misrepresents variable state is actively misleading. For \textbf{iterative self-refinement}, feeding a model its own flawed reasoning causes compounding errors. For \textbf{agentic workflows}, a plan based on unsound understanding of a previous step can derail an entire task. Figure~\ref{fig:motivation} illustrates this critical problem concretely.

\begin{figure*}[t]
\centering
\footnotesize
\definecolor{initcolor}{RGB}{255,250,205}
\definecolor{processcolor}{RGB}{230,240,255}
\definecolor{finalcolor}{RGB}{230,255,230}
\setlength{\tabcolsep}{2pt}
\begin{tabular}{@{}p{0.22\textwidth}@{\hspace{0.005\textwidth}|}@{\hspace{0.005\textwidth}}p{0.24\textwidth}@{\hspace{0.005\textwidth}|}@{\hspace{0.005\textwidth}}p{0.22\textwidth}@{\hspace{0.005\textwidth}|}@{\hspace{0.005\textwidth}}p{0.24\textwidth}@{}}
\toprule
\textit{Code} & \textit{LLM-Generated CoT} & \textit{Execution Trace} & \textit{Our CoT} \\
\midrule
\begin{lstlisting}[style=pythoncode, basicstyle=\ttfamily\scriptsize]
def find_peak(arr):
  left = 0
  right = len(arr)-1
  while left < right:
    mid=(left+right)//2
    if arr[mid]<arr[mid+1]:
      left = mid + 1
    else:
      right = mid
  return left
# Call: find_peak([1,3,5,4,2])
\end{lstlisting}
&
\colorbox{errorred!8}{\parbox{0.96\linewidth}{\scriptsize
The function begins by initializing search boundaries spanning the array. With left=0 and right=4, we start looking for the peak. In the loop, we calculate the midpoint and check if we're ascending or descending. At mid=2, we compare arr[2] with \textcolor{errorred}{\textbf{arr[1]}} to determine direction. Since arr[2] is greater, we're still climbing, meaning the peak lies to the \textcolor{errorred}{\textbf{right}}, so we adjust our boundary accordingly. We repeat until the boundaries meet at the peak index.\\[0.1cm]
\textcolor{errorred}{\xmark\ Hallucinates indices \& direction}
}}
&
\colorbox{gray!15}{\parbox{0.96\linewidth}{\ttfamily\tiny
\colorbox{initcolor}{\textbf{Initialization}}\\
Line 2: left = 0\\
Line 3: right = 4\\[0.05cm]
\colorbox{processcolor}{\textbf{Loop Iteration 1}}\\
Line 5: mid = 2\\
Line 6: arr[2]=5, arr[3]=4\\
Line 9: right: 4$\rightarrow$2\\[0.05cm]
\colorbox{processcolor}{\textbf{Loop Iteration 2}}\\
Line 5: mid = 1\\
Line 6: arr[1]=3, arr[2]=5\\
Line 7: left: 0$\rightarrow$2\\[0.05cm]
\colorbox{finalcolor}{\textbf{Return}}\\
Line 10: return 2
}}
&
\colorbox{correctgreen!8}{\parbox{0.96\linewidth}{\scriptsize
\colorbox{initcolor}{\parbox{0.94\linewidth}{\scriptsize Starting with array [1,3,5,4,2], we initialize left=0 and right=4.}}\\[0.03cm]
\colorbox{processcolor}{\parbox{0.94\linewidth}{\scriptsize First iteration finds midpoint at index 2 (value 5). Comparing with arr[3]=4 shows we're past the peak, so we set right=2 to search the left portion.}}\\[0.03cm]
\colorbox{processcolor}{\parbox{0.94\linewidth}{\scriptsize Second iteration has mid=1 (value 3). Comparing with arr[2]=5 shows the peak is ahead, so left moves to 2.}}\\[0.03cm]
\colorbox{finalcolor}{\parbox{0.94\linewidth}{\scriptsize Boundaries converge at index 2, which holds our peak value of 5.}}\\[0.05cm]
\textcolor{correctgreen}{\cmark\ Verified step-by-step}
}}
\\
\bottomrule
\end{tabular}
\vspace{-0.1cm}
\caption{Comparison of hallucinated vs trace-grounded CoT. Our approach translates pysnooper execution trace (showing line numbers, variable states, state transitions) into natural language reasoning. Color mapping shows trace sections: \colorbox{initcolor}{yellow=init}, \colorbox{processcolor}{blue=iterations}, \colorbox{finalcolor}{green=return}. LLM hallucinates wrong comparison (arr[2] vs arr[1] instead of arr[3]) and wrong search direction.}
\label{fig:motivation}
\end{figure*}

% This challenge is particularly acute in code reasoning because small logical errors can cascade through multi-step reasoning processes, leading to flawed understanding. For instance, a model might correctly identify that a function performs a sorting operation but incorrectly reason about the intermediate steps, variable updates, or edge case handling. Such errors in reasoning data propagate during training, teaching models to follow superficially plausible but ultimately incorrect logical patterns. For instance, Other 
%      execution-aware methods, used in benchmarks like HumanEval~\cite{chen2021evaluating} or by prior work like 
%      SemCoder~\cite{ding2024semcodertrainingcodelanguage}, only perform \textit{outcome verification}. They check that 
%      the final result of a program or rationale is correct, but they do not verify the logical integrity of the 
%      intermediate reasoning steps. This leaves a critical gap for a method that can guarantee the soundness of the 
%      entire reasoning chain.
% This challenge is especially acute in code reasoning, where small logical errors can cascade into a flawed understanding. For instance, a model may correctly identify a function's purpose but hallucinate the logic for its intermediate steps or edge cases. Such errors then propagate during training, teaching the model to follow plausible but incorrect logical patterns.

Existing work uses execution in two ways. Benchmarks like HumanEval~\cite{chen2021evaluating} and methods like SemCoder~\cite{ding2024semcodertrainingcodelanguage} validate that the final program output is correct, but do not verify the logical integrity of intermediate reasoning steps. Formal verification methods can provide mathematical guarantees but are computationally prohibitive at scale. \textbf{A critical gap exists:} there is no practical method for validating each step in the reasoning chain—what we term \textit{rationale-step verification}—that scales to large-scale training data generation. Additionally, existing pipelines lack fine-grained control over problem complexity and diversity.

Furthermore, effective code reasoning requires both forward inference (predicting outputs from inputs) and backward reasoning (understanding how outputs arose from inputs)~\cite{li2025codeio}. This bi-directional capability is essential for debugging, code comprehension, and robust program analysis. However, current synthetic data generation methods do not systematically address this requirement, leading to models with asymmetric reasoning capabilities.

In this work, we bridge these gaps by introducing a new paradigm for data synthesis that achieves \textit{rationale-step verification} at scale. We have developed a complete synthesis pipeline that generates CoT rationales by directly translating program execution traces into natural language. Specifically, we instrument a program to capture its dynamic execution state—recording variable values after each statement, control flow decisions, and state transitions derived from program semantic operations. This trace serves as an immutable ground truth from which rationales are derived. Each step in the resulting reasoning chain can be verified against the trace to confirm: (1) variable values match recorded runtime values, (2) state transitions correctly reflect how variables changed between statements, and (3) control flow accurately describes which branches executed. This ensures rationales are \textbf{correct by construction}, eliminating the logical hallucinations that plague other methods.

Furthermore, our \textit{concept-first} pipeline provides fine-grained control over data quality. Rather than
      relying on existing code corpora, it synthesizes problems from abstract programming concepts (e.g.,
      "pass-by-reference semantics," "dynamic programming") extracted from technical literature. This allows us to
      systematically control problem difficulty and ensure diverse coverage of algorithmic and mathematical domains.
      Combined with bi-directional trace generation, this provides a complete foundation for training models on both
      forward reasoning (predicting outputs) and backward reasoning (diagnosing how outputs arose), with all rationales
      grounded in verifiable execution.


Our contributions are fivefold:

\begin{enumerate}
    \item \textbf{Rationale-Step Verification at Scale}: We introduce the first execution-grounded synthesis pipeline that achieves rationale-step verification—validating each reasoning step—at scale. By directly translating program execution traces into natural language, we generate CoT rationales that are \textbf{correct by construction}, with formal guarantees that variable values, state transitions, and control flow are verifiable against actual program behavior.

    \item \textbf{Concept-First Curriculum Synthesis}: A novel curriculum-driven generation approach that extracts programming concepts from technical literature rather than mining existing code corpora. This provides fine-grained control over problem complexity and algorithmic diversity, enabling systematic coverage from basic constructs to advanced algorithms.

    \item \textbf{Bi-Directional Reasoning Dataset}: The first large-scale dataset (54,000 samples) systematically teaching both forward (input→output) and backward (output→input) reasoning with trace-grounded verification. This bi-directional capability is essential for debugging, code comprehension, and robust program analysis.

    \item \textbf{Comprehensive Empirical Validation}: Through systematic ablations, we demonstrate that verification quality directly determines model performance. Models trained on our trace-grounded data achieve +21.9 points over baselines on LiveCodeBench-Exec, with analysis showing LLM-generated explanations perform 8.9 points worse due to lack of execution grounding.

    \item \textbf{Open-Source Pipeline}: Complete synthesis infrastructure including concept extraction, hierarchical generation, Dual Agreement verification, and trace-to-rationale translation to facilitate reproducible research in verified reasoning for code.
\end{enumerate}

The remainder of this paper is organized as follows. We first review related work in code reasoning and synthetic data generation. Next, we detail our three-stage data synthesis pipeline, followed by our experimental design and a thorough analysis of the results. We conclude with a discussion of our findings and future research directions.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.95\textwidth]{cot_diagram_v4.pdf}
    \caption{An overview of our three-stage data synthesis pipeline. 
    \textbf{Stage A (Concept Sourcing \& Synthesis):} Generates candidate concepts, code, and tests from raw documents. 
    \textbf{Stage B (Verification \& Clustering):} Uses our execution-based Dual Agreement algorithm to identify and rank the highest-quality solution-test pairs. 
    \textbf{Stage C (CoT Generation):} Uses the verified artifacts and their execution traces to produce the final, bi-directional conversational data, complete with trace-grounded rationales and feedback.}
    \label{fig:system_overview}
\end{figure*}

\begin{table*}[t!]
\centering
\caption{Comparison of features in related works on code reasoning data and methodologies. Our work is the first to systematically combine execution-grounded verification with bi-directional CoT generation at scale.}
\label{tab:related_work_comparison}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|ccccc}
\toprule
\textbf{Work / Method} & \textbf{Execution-Trace} & \textbf{Natural Language} & \textbf{Bi-Directional} & \textbf{Verifies} & \textbf{Scalable Data} \\ 
& \textbf{Grounded CoT} & \textbf{CoT} & \textbf{CoT} & \textbf{Output} & \textbf{Synthesis} \\
\midrule
REVTHINK~\cite{chen2024reverse} & \xmark & \cmark & \cmark & \xmark & \cmark \\
TRACED~\cite{ding2024traced} & \xmark & \xmark & \xmark & \cmark & \cmark \\
CodeI/O~\cite{li2025codeio} & \xmark & \cmark & \cmark & \cmark & \cmark \\
rStar-Coder~\cite{liu2025rstarcoderscalingcompetitivecode} & \xmark & \cmark & \xmark & \cmark & \cmark \\
Jung et al.~\cite{jung2025codeexecutiongroundedsupervision} & \cmark & \cmark & \xmark & \cmark & \xmark \\
SemCoder~\cite{ding2024semcodertrainingcodelanguage} & \xmark & \cmark & \cmark & \cmark & \cmark \\
\midrule 
\textbf{Our Work} & \textbf{\gcmark} & \textbf{\gcmark} & \textbf{\gcmark} & \textbf{\gcmark} & \textbf{\gcmark} \\
\bottomrule
\end{tabular}% 
}
\end{table*}




\section{Related Work}
Our work builds upon several interconnected research areas in code reasoning, execution-based program understanding, and synthetic data generation. Chain-of-Thought (CoT) prompting~\cite{wei2022chain} has proven effective for improving reasoning, but its extension to code faces unique verification challenges. While methods like Program-of-Thought (PoT)~\cite{chen2022program} generate executable snippets, they lack systematic verification of the reasoning process itself. Building on this foundation, CodeI/O~\cite{li2025codeio} takes a significant step forward by transforming code patterns into natural language CoT rationales through input/output prediction tasks. By training models to predict inputs and outputs given code, CodeI/O exposes models to universal reasoning primitives while decoupling reasoning from syntax. However, CodeI/O focuses on prediction tasks rather than generating complete reasoning datasets from execution traces.

\textbf{Verification Approaches in Code Generation.} The term "verification" appears frequently in code generation literature but often refers to different concepts. We identify three distinct approaches: (1) \textbf{Outcome Verification} validates only the final program output against expected results. This is the most common approach, used by benchmarks like HumanEval~\cite{chen2021evaluating} and methods like SemCoder~\cite{ding2024semcodertrainingcodelanguage} and rStar-Coder~\cite{liu2025rstarcoderscalingcompetitivecode}. While effective for validating solutions, it offers no guarantee about the logical integrity of intermediate reasoning steps. (2) \textbf{Formal Verification} uses mathematical proofs (e.g., symbolic execution, SMT solvers) to guarantee program correctness. While sound, this approach is computationally heavyweight and impractical for large-scale LLM training data generation. (3) \textbf{Rationale-Step Verification} (our contribution) validates each reasoning step in the CoT chain. By generating rationales directly from a program's execution trace—a factual record capturing variable values, state transitions, and control flow at runtime—we can verify that each reasoning step correctly describes: (a) the variable values at that point in execution, (b) how those values changed from the previous state due to program semantic operations, and (c) which control flow paths were taken. This verifies the logical integrity of the entire reasoning process, not just the final outcome.

Recent work like TRACED~\cite{ding2024traced} and Execution Tuning (E.T.)~\citep{armengol2025execution} has explored using execution traces for pre-training, but these methods do not generate natural language rationales with step-by-step verification. Our work is the first to systematically achieve rationale-step verification at scale by translating execution traces into verifiable natural language CoT.

\textbf{Comparison to SemCoder.} A highly relevant work, SemCoder~\cite{ding2024semcodertrainingcodelanguage}, also generates synthetic CoT data for fine-tuning. However, our methodology differs fundamentally in two aspects. First, SemCoder generates rationales via an LLM's \textit{explanation} of code behavior, then verifies only the final predicted output. We perform a direct \textit{translation} of execution traces into natural language, making each intermediate reasoning step verifiable: claims about variable values can be checked against recorded runtime values, state transitions can be confirmed against logged changes between statements, and control flow descriptions can be validated against the actual execution path. This narration-vs-generation distinction eliminates logical hallucinations. Second, SemCoder uses a \textit{code-first} approach seeded by existing programs. Our \textit{concept-first} approach synthesizes problems from abstract concepts extracted from technical literature, providing fine-grained control over complexity and diversity. Conceptually closest to our trace translation, Jung et al.\cite{jung2025codeexecutiongroundedsupervision} generate natural language CoT from execution traces, but focus on single-direction reasoning rather than our systematic bi-directional pipeline.

\textbf{Comparison to rStar-Coder.} rStar-Coder~\cite{liu2025rstarcoderscalingcompetitivecode} scales up verified data for competitive programming through iterative generation and execution-based verification. While both leverage execution, rStar-Coder verifies the \textit{final outcome} of generated code, whereas we verify \textit{every intermediate reasoning step} by generating rationales directly from traces. Consequently, rStar-Coder's rationales are LLM-generated explanations (subject to hallucination), while ours are correct by construction. In summary, rStar-Coder excels at verifiably correct \textit{solutions}; we create verifiably correct \textit{reasoning processes}.

\textbf{Bi-Directional Reasoning.} The principle of bi-directional reasoning has been explored in code by CodeI/O~\cite{li2025codeio} through I/O prediction tasks, and in other domains by FOBAR~\cite{jiang2024forward} and REVTHINK~\cite{chen2024reverse}. However, these approaches focus on prediction tasks rather than generating complete trace-grounded rationales. Other verification techniques like Self-Verification~\cite{weng2023large} and RCoT~\cite{xue2023rcot} rely on complex checking procedures, while our deterministic trace-based method provides guaranteed correctness.

As summarized in Table \ref{tab:related_work_comparison}, no existing work provides a systematic pipeline for generating bi-directional CoT datasets where every reasoning step is grounded in verifiable execution. Our work addresses this gap, providing a trustworthy foundation for training robust code reasoning capabilities.



\section{Data Synthesis Pipeline}
Our data synthesis pipeline is a multi-stage process designed to generate high-fidelity, bi-directional CoT data where every reasoning step is anchored in ground-truth program execution. This ensures that the resulting dataset is correct by construction, eliminating the logical hallucinations common in purely model-generated rationales.

\subsection{Stage A: Concept Sourcing and Curriculum-Driven Synthesis}
Rather than generating code from simple prompts, our pipeline begins by building a curriculum of programming concepts derived from high-quality sources. This curriculum-driven approach ensures the resulting problems are complex, diverse, and grounded in established knowledge.

\subsubsection{Document Processing and Concept Extraction}
Our pipeline processes a diverse corpus of permissively-licensed technical literature, including books from the StarCoder2-documentation dataset~\cite{starcoder2-doc} and curated programming resources spanning basic to advanced topics. Rather than relying on raw PDF text extraction, which produces noisy output, we employ \textbf{Docling}, a document understanding framework that renders PDFs into clean, structured markdown. This preprocessing preserves semantic structure (headings, code blocks, lists), removes pagination artifacts, and maintains proper formatting. The cleaned text is chunked into 4000-character segments with sliding overlap to prevent concept fragmentation.

\subsubsection{Hybrid Concept Identification}
For each text chunk, we employ a three-stage hybrid extraction strategy balancing recall with precision:

\textbf{Stage 1: Statistical Keyword Extraction.} We use spaCy with PyTextRank, a graph-based ranking algorithm, to identify candidate concepts. PyTextRank applies the TextRank algorithm (adapted from PageRank) to the chunk's lemma graph, where nodes represent terms and edges represent co-occurrence relationships. This produces a ranked list of phrases based on their centrality. However, this captures non-technical terms (e.g., page numbers, dates) and misses implicit concepts discussed narratively.

\textbf{Stage 2: LLM-Based Filtering and Augmentation.} To address these limitations, we prompt an LLM teacher model (Qwen2.5-Coder-7B-Instruct) with a filtering prompt that: (1) removes book metadata and non-Python-specific terms, (2) completes incomplete phrases, and (3) identifies implicit concepts present in text but not captured statistically. This hybrid approach yields approximately 15,000 initial concepts.

\textbf{Why This Hybrid Approach?} Pure NLP methods lack semantic understanding and generate noise; pure LLM extraction is expensive and may hallucinate concepts not in text. Our hybrid leverages the efficiency of statistical methods for recall and LLM semantic reasoning for precision.

\subsubsection{Rigorous Concept Deduplication and Quality Scoring}
The initial 15,000 concepts contain substantial redundancy and quality variance. We apply multi-stage quality control:

\textbf{Deduplication.} We normalize concepts (lowercasing, lemmatizing) and perform string similarity matching to cluster near-duplicates, selecting the most complete variant as canonical.

\textbf{LLM-Based Quality Scoring.} For each deduplicated concept, we prompt the teacher model to score along two dimensions: (1) \textbf{Difficulty} (1-5): algorithmic complexity required for implementation, and (2) \textbf{Relevance} (1-5): ability to inspire diverse, non-trivial problems. We retain only concepts scoring $\geq 3$ on both dimensions, yielding approximately 8,000 high-quality seed concepts.

\textbf{Why Difficulty Filtering Matters.} Preliminary experiments revealed that naively sampling produces a distribution heavily skewed toward trivial problems. This is problematic because: (1) trivial problems do not challenge multi-step reasoning, (2) resulting CoT rationales are too short for meaningful supervision, and (3) downstream verification struggles with trivial problems where nearly all candidates pass all tests. Difficulty filtering biases the curriculum toward problems requiring algorithmic thinking.

\subsubsection{Hierarchical Problem Synthesis}
For each high-quality concept, we synthesize a complete problem artifact set through a carefully ordered five-step prompting pipeline. The ordering is designed to \textit{constrain the solution space at each step}, preventing type and naming inconsistencies.

\textbf{Step 1: Instruction Generation.} Given a concept and its description, we prompt the teacher model to generate 6 distinct natural language problem instructions. The prompt is engineered for diversity along three axes: (1) problem domain (mathematics, finance, data processing, algorithms), (2) computational approach (iterative, recursive, dynamic programming, functional), and (3) difficulty (3 "medium" targeting 30-60 LOC, 3 "hard" targeting 50-100+ LOC). The prompt includes explicit anti-overlap guidance to avoid paraphrased versions of the same problem.

\textbf{Step 2: Signature Generation.} Before generating any code or tests, we prompt the LLM to analyze the instruction and produce a formal signature skeleton specifying: (1) implementation type (standalone function vs. class), (2) function signature with parameter types and return type, or (3) class signature with constructor and methods. The signature prompt uses a strict formatting template enforced via few-shot examples, as shown in Figure~\ref{fig:signature-formats}. This signature-first design is deliberate: once input/output types and function names are fixed, subsequent code generation cannot hallucinate different types or names, eliminating import failures and ensuring test-code alignment.

\textbf{Step 3: Code Generation.} With the signature locked, we generate 5 candidate solutions per instruction by prompting with both the instruction and signature. The prompt explicitly instructs: (1) strictly adhere to the signature, (2) vary computational approach across implementations, and (3) ensure completeness.

\textbf{Step 4: Test Scenario Identification.} We prompt the LLM to analyze the problem and produce a concise list of test scenarios (e.g., "Test basic functionality", "Test empty input"). This improves test coverage by encouraging reasoning about requirements before writing code.

\textbf{Step 5: Test Generation.} We generate unit tests (3 test suites per instruction, each with up to 10 test functions) using a meticulously engineered prompt enforcing strict structural constraints. Each test is a top-level function (not a class method) containing exactly one assert statement with direct function calls using inline arguments—no variable assignments. Figure~\ref{fig:test-formats} illustrates the correct format versus prohibited patterns. These constraints are critical for Stage B's granular pass/fail clustering and Stage C's clean trace extraction, while enabling parallel sandboxed execution.

At the end of Stage A, we have for each concept: 6 instructions, each with 1 signature, 5 candidate solutions, and 3 test suites, totaling approximately 85,000 candidate instruction-solution-test triples.

\input{listings}

\subsection{Stage B: Execution-Based Verification and Agreement Clustering}
The hierarchical generation in Stage A, while carefully constrained, inevitably produces noise: some candidate solutions are incorrect (due to LLM generation errors), some tests are malformed or have wrong expected outputs, and some instruction-signature pairs are ambiguous. Before generating expensive CoT rationales in Stage C, we must identify and discard low-quality artifacts. We adapt the Dual Agreement verification methodology from CodeT~\cite{chen2022codetcodegenerationgenerated}, originally designed for single-solution selection, into a scalable batch filtering algorithm.

\subsubsection{Mass Execution and Pass/Fail Matrix Construction}
For each problem (identified by \texttt{task\_id}), we have $m$ candidate solutions and $n$ candidate tests. We execute all $m \times n$ solution-test pairs in a secure, sandboxed environment (Docker containers with resource limits) to construct a binary pass/fail matrix $M \in \{0, 1\}^{m \times n}$, where $M[i, j] = 1$ if solution $i$ passes test $j$, and $0$ otherwise. Execution failures (timeouts, runtime errors, assertion failures) are all treated as $M[i, j] = 0$. This brute-force execution is computationally expensive ($O(m \cdot n)$ per task) but embarrassingly parallel.

\subsubsection{Dual Agreement Clustering}
The core insight of Dual Agreement is: \textit{If a large set of independently generated solutions all pass the same large set of independently generated tests, it is statistically unlikely that both the solutions and tests are incorrect.} We formalize this as follows:

\textbf{Clustering by Test Agreement.} We partition the $m$ candidate solutions into clusters $\{C_1, C_2, \ldots, C_k\}$ such that all solutions within a cluster $C_i$ have \textit{identical} pass/fail patterns across all $n$ tests. Formally, solutions $s_a$ and $s_b$ are in the same cluster iff:
\[
\forall j \in [1, n]: M[a, j] = M[b, j]
\]
This is implemented via hash-based grouping: we compute a fingerprint for each solution's test pass pattern (bit vector hash), then group solutions by fingerprint.

\textbf{Cluster Scoring.} For each cluster $C_i$, let $T_p(C_i)$ denote the set of tests that all solutions in $C_i$ pass. We assign a quality score:
\[
\text{Score}(C_i) = |C_i| \times |T_p(C_i)|
\]
This score captures two dimensions: (1) \textit{Solution agreement} ($|C_i|$): If many independently generated solutions have identical behavior, they likely implement the same correct algorithm. (2) \textit{Test coverage} ($|T_p(C_i)|$): If they pass many tests, the tests likely have correct expected outputs.

\textbf{Best Cluster Selection.} For each task, we rank clusters by score and select the \textbf{single highest-scoring cluster}. We extract one canonical solution from this cluster (selecting the shortest or most readable) and its associated passing tests. Algorithm~\ref{alg:dual_agreement} formalizes this procedure. This method effectively filters multiple noise sources: incorrect solutions land in low-scoring clusters due to failing many tests (low $|T_p|$), malformed tests reduce scores but cannot eliminate correct clusters if other valid tests exist, and ambiguous instructions cause solutions to fragment into multiple small clusters (low $|C_i|$), with the highest-scoring cluster representing the most common interpretation.

\begin{algorithm}[h]
\caption{Dual Agreement Clustering}
\label{alg:dual_agreement}
\begin{algorithmic}[1]
\REQUIRE Solutions $S = \{s_1, \ldots, s_m\}$, Tests $T = \{t_1, \ldots, t_n\}$
\ENSURE Verified solution $s^*$ and test suite $T^*$
\STATE Execute all $m \times n$ pairs to build matrix $M \in \{0,1\}^{m \times n}$
\STATE $\textit{clusters} \gets \emptyset$
\FOR{each solution $s_i \in S$}
    \STATE $\textit{pattern}_i \gets M[i, :]$ // Test pass/fail pattern
    \STATE Add $s_i$ to cluster with matching pattern in \textit{clusters}
\ENDFOR
\FOR{each cluster $C_i \in \textit{clusters}$}
    \STATE $T_p(C_i) \gets \{t_j : \forall s \in C_i, M[s, j] = 1\}$ // Commonly passed tests
    \STATE $\textit{Score}(C_i) \gets |C_i| \times |T_p(C_i)|$
\ENDFOR
\STATE $C^* \gets \arg\max_{C_i} \textit{Score}(C_i)$ // Highest-scoring cluster
\STATE $s^* \gets$ Select canonical solution from $C^*$ (shortest/most readable)
\STATE $T^* \gets T_p(C^*)$
\STATE \textbf{return} $(s^*, T^*)$
\end{algorithmic}
\end{algorithm}

\subsubsection{Theoretical Justification: Why Multiple Candidates Matter}
The effectiveness of Dual Agreement relies critically on generating multiple diverse candidates. While a single solution-test pair provides no signal about correctness, multiple independently generated candidates enable statistical consensus detection through two complementary properties.

\textbf{Unlikely Collision Property.} The probability that $K$ independently incorrect solutions would \textit{all} pass the same incorrect test suite decreases exponentially with $K$. Formally, if each incorrect solution has probability $p$ of coincidentally passing an incorrect test, then the probability that all $K$ incorrect solutions pass is $p^K$. For reasonable values (e.g., $p = 0.3$, $K = 5$), this yields $p^K = 0.00243$, making such false consensus highly unlikely. In contrast, $K$ correct solutions deterministically pass correct tests. This means large $|C_i|$ (solution agreement) and large $|T_p(C_i)|$ (test coverage) together provide strong correctness signals—clusters with \textit{both} high $|C_i|$ and high $|T_p|$ survive the scoring threshold, filtering out isolated or weakly-validated artifacts.

\textbf{Mathematical Formulation of Noise Reduction.} We can formalize why this scoring creates clear separation between correct and incorrect clusters. Let $C_{correct}$ be the cluster of correct solutions and $C_{error}$ be any cluster of incorrect solutions. For the correct cluster:
\[
\mathbb{E}[\text{Score}(C_{correct})] = \mathbb{E}[|C_{correct}|] \times n_{valid}
\]
where $n_{valid}$ is the number of valid tests. For an incorrect cluster:
\[
\mathbb{E}[\text{Score}(C_{error})] \leq \mathbb{E}[|C_{error}|] \times n_{valid} \times (1 - \delta)
\]
where $\delta > 0$ represents the fraction of tests that expose the bug. When we have $K$ diverse candidate solutions with $m$ correct among them, the expected score ratio becomes:
\[
\frac{\mathbb{E}[\text{Score}(C_{correct})]}{\mathbb{E}[\text{Score}(C_{error})]} \geq \frac{m}{(K-m)(1-\delta)}
\]
As $K$ increases with sufficient diversity, $m$ (correct solutions) grows while incorrect solutions fragment into multiple small clusters with different error patterns, each with low $|C_{error}|$. This multiplicative scoring creates exponential separation, allowing correct clusters to dominate even with noisy candidates. However, this requires diversity: generating varied candidates via temperature sampling, prompt variation, and varied computational approaches in Stage A ensures that correct implementations converge while incorrect ones fragment. The effectiveness of this approach is empirically validated in Section~\ref{sec:dual_agreement_validation}.

\subsubsection{Scale and Filtering Results}
From the initial approximately 85,000 candidate triples, Dual Agreement verification identifies approximately 85,000 verified solution-test pairs (one per task). A final deduplication pass (using code similarity hashing) yields our \textbf{master dataset of approximately 85,000 high-confidence, verified artifacts}, each consisting of: a concept and instruction, a verified solution, a verified test suite, and signature metadata.


% \subsection{Stage C: Execution-Grounded CoT Generation}
% The final stage transforms the verified solution-test pairs from Stage B into a rich, conversational dataset. This is not a single-prompt process, but a meticulous, automated pipeline that builds each component of the forward and backward reasoning chains.


% \begin{enumerate}
%     \item \textbf{Execution Tracing:} For each verified solution and its set of passing test cases, we first generate a ground-truth execution trace. We use \texttt{pysnooper} to instrument the specific function or method identified in the problem's signature information. The solution is then executed against each individual test case in a secure, sandboxed process with a strict timeout. This produces a raw, line-by-line trace file detailing every variable assignment and state change for each successful execution.

%     \item \textbf{Trace Sanitization:} The raw \texttt{pysnooper} logs contain formatting artifacts and ANSI color codes not suitable for model consumption. A dedicated script cleans each trace file, removing these artifacts to produce a clean, plain-text representation of the program's execution flow.

%     \item \textbf{Bi-Directional Conversation Synthesis:} This is the core of the stage, where the cleaned trace is used to generate the final conversational data. For each test case, the pipeline performs the following sequence of LLM-driven steps:
%     \begin{itemize}
%         \item \textbf{Ground-Truth I/O Extraction.} The pipeline first establishes the precise input and output for the test case. 
%         % It intelligently categorizes tests: for simple, standalone \texttt{assert} statements, it uses regular expressions to directly parse the function call input and the expected output. For more complex tests involving setup code, it uses a targeted LLM prompt to analyze the test and extract the effective input/output pair.
        
%         \item \textbf{Question Generation.} With the ground-truth input and output established, an LLM is prompted to generate a pair of natural language questions: a \textbf{forward question} (e.g., "Given the input `[1, 2]`, what does the function return?") and a corresponding \textbf{backward question} (e.g., "What input would cause the function to return `3`?").
        
%         \item \textbf{Trace-Grounded CoT Generation.} The cleaned execution trace is now used as the factual basis for generating the reasoning chains.
%         \begin{itemize}
%             \item For the \textit{forward question}, an LLM is prompted to narrate the execution trace, explaining step-by-step how the given input is transformed into the final output. This narrative forms the forward Chain-of-Thought. The prompt explicitly instructs the model to extract the \texttt{<Predicted Output>} it deduces from the trace.
%             \item For the \textit{backward question}, a separate LLM prompt asks the model to perform deductive reasoning. Using the trace as evidence, it must explain how the final output state could only have been reached from the initial input. This forms the backward Chain-of-Thought, and the model is asked to extract the \texttt{<Predicted Input>} it derives. Refer~\ref{fig:cot-templates}.
%         \end{itemize}
        
%         % \item \textbf{Verifiable Feedback Generation.} The pipeline creates a final layer of verification. The `predicted\_output` from the forward CoT is compared to the ground-truth output, and an LLM generates a \textbf{feedback response} confirming the match and explaining why it is correct based on the problem. The same process is repeated for the `predicted\_input` from the backward CoT.
%     \end{itemize}

%     \item \textbf{Final Assembly:} All the generated components—the questions, the trace-grounded CoTs, the predicted I/O-are assembled into conversational format. 
%     % The final output is a JSONL file where each line represents a complete, bi-directional reasoning conversation, grounded in verifiable program execution.
% \end{enumerate}


\subsection{Stage C: Execution-Grounded CoT Generation}
The final stage transforms each verified solution-test pair into a rich, conversational training dataset where every reasoning step is grounded in the program's actual execution. This is not a single-prompt process, but a meticulous five-step automated pipeline.

\subsubsection{Step 1: Execution Trace Generation}
For each verified solution and each of its passing test cases, we generate a ground-truth execution trace capturing the program's dynamic behavior. We use \texttt{pysnooper}, a lightweight Python tracing library, to instrument the target function (identified via signature metadata from Stage A).

\textbf{Instrumentation Strategy.} Rather than tracing the entire program (which includes test framework overhead), we selectively instrument only the function under test. For a function \texttt{solution}, we decorate it with \texttt{@pysnooper.snoop(output=trace\_file)}, then execute the test case in a sandboxed subprocess with a strict timeout (10 seconds). This produces a line-by-line trace file detailing: (1) \textit{Call events:} Function entry with input arguments, (2) \textit{Line execution:} Each line number executed in sequence, (3) \textit{Variable modifications:} Every assignment with variable name, old value, and new value (capturing state transitions), and (4) \textit{Return events:} Function exit with return value.

This trace is the immutable ground truth: a factual record of the program's operational semantics—what values variables held at each point, how those values changed from statement to statement, and which execution paths were taken. It is not an LLM's explanation or interpretation, but the actual runtime behavior.

\subsubsection{Step 2: Trace Sanitization and Normalization}
Raw \texttt{pysnooper} logs contain formatting artifacts unsuitable for model consumption: ANSI color codes, timestamp prefixes, file path prefixes, and redundant whitespace. A dedicated sanitization script strips these artifacts via regex-based cleaning, producing a clean, plain-text trace representation. This is critical because LLMs are sensitive to formatting: spurious tokens can degrade generation quality.

\subsubsection{Step 3: Ground-Truth I/O Extraction and Question Generation}
Before generating CoT rationales, we establish the precise input and output for each test case. For simple test functions like \texttt{def test\_basic(): assert solution([1, 2], 2) == [1]}, we use regex parsing to extract \texttt{input = [1, 2], 2} and \texttt{expected\_output = [1]}. For complex tests involving setup logic, we prompt an LLM to analyze the test code and extract the effective input/output pair. With ground-truth I/O established, we then prompt an LLM to generate a pair of natural language questions: (1) \textbf{Forward question:} "Given the input \texttt{[1, 2, 3]} and \texttt{target=2}, what does the function return?", and (2) \textbf{Backward question:} "What input would cause the function to return \texttt{[1]}?" The prompt explicitly instructs natural phrasing to add linguistic diversity to the dataset.

\subsubsection{Step 4: Trace-Grounded Chain-of-Thought Generation}
This is the core of Stage C, where we use the sanitized trace as the factual basis for generating reasoning chains.

\textbf{Forward CoT Generation: Narrating the Trace.} We prompt an LLM with: (1) the instruction and function code, (2) the forward question, and (3) the sanitized execution trace. The prompt instructs: "Narrate the execution trace step-by-step, explaining how the input is transformed into the output. Extract the \texttt{<Predicted Output>} you deduce from the trace." Critically, the model is \textit{not generating an explanation from scratch}; it is \textit{translating} a factual trace into natural language. Each statement in the rationale can be verified against the trace: variable values mentioned must match the recorded runtime values, state transitions described must correspond to logged changes between statements, and the sequence of operations must follow the actual control flow path executed.

\textbf{Backward CoT Generation: Deductive Reasoning from the Trace.} For the backward direction, we prompt the model with the same trace but ask it to perform deductive reasoning: "Given the final output state in the trace, explain how this output could only have been reached from the initial input. Extract the \texttt{<Predicted Input>} you derive." This forces the model to reason backwards through the trace, explaining how the final state constrains the initial input.

\textbf{Why This Trace-Grounding Eliminates Hallucination.} Traditional self-instruct methods prompt an LLM to explain code without execution, leading to "plausible hallucinations" where the explanation sounds correct but contains subtle factual errors (e.g., claiming a variable was incremented when it was decremented). By anchoring the rationale in a trace, we guarantee factual correctness: (1) \textit{Variable values are verifiable:} Each value mentioned in the rationale is recorded at runtime and can be cross-referenced against the trace log. (2) \textit{State transitions are verifiable:} Claims about how variables changed between statements (e.g., "the counter increases from 0 to 1") can be confirmed by comparing consecutive trace entries showing the variable's old and new values. (3) \textit{Control flow is verifiable:} Descriptions of which branches or loops executed can be validated against the trace's record of which lines were executed and in what order. This ensures the entire causal chain—from initial state through semantic operations to final state—is factually grounded.

\subsubsection{Step 5: Final Assembly into Conversational Format}
The generated components (questions, CoTs, predicted I/O) are assembled into structured training examples. We create three versions of the dataset: (1) \textbf{Forward-only:} Each sample contains \texttt{<Instruction>}, \texttt{<Function>}, \texttt{<Forward Question>}, \texttt{<CoT>}, \texttt{<Predicted Output>}; (2) \textbf{Backward-only:} Each sample contains \texttt{<Instruction>}, \texttt{<Function>}, \texttt{<Backward Question>}, \texttt{<CoT>}, \texttt{<Predicted Input>}; (3) \textbf{Bi-directional:} Each sample contains both forward and backward turns in a single multi-turn conversation, teaching the model to reason in both directions.

This yields our final dataset of 54k verified, trace-grounded, bi-directional reasoning samples.

\section{Experimental Design}
Our experiments are designed to rigorously quantify the benefits of our execution-grounded data and to determine the optimal configurations for fine-tuning. 
% We adopt a systematic, top-down ablation approach, first identifying the most effective data curation strategy and then the best reasoning format. This entire process is conducted on two distinct model families to evaluate both efficacy and generalizability.

% \subsection{Models and Benchmarks}
% Our experiments use two open-source base models of a similar scale: \texttt{granite-3.3-8b-base}~\cite{granite3.3-base} as our primary model and \texttt{Qwen2.5-Coder-7B}~\cite{hui2024qwen2} to validate the generalization of our approach on a distinct, coding-focused architecture. We evaluate all models on LiveCodeBench (Execution)~\cite{jain2024livecodebench} and CruxEval (Input/Output)~\cite{gu2024cruxeval} benchmarks specifically chosen for their focus on code execution reasoning, such as predicting outputs from inputs and, in the case of CruxEval, vice versa.

\subsection{Models and Benchmarks}
Our model selection was deliberately designed to test our method across two open-source foundation models of a similar scale but with distinct pre-training philosophies. We selected \texttt{granite-3.3-8b-base}~\cite{granite3.3-base}, an enterprise-grade architecture trained exclusively on permissively licensed data, which validates our approach for real-world applications where data provenance is critical. Alongside it, we chose \texttt{Qwen2.5-Coder-7B}~\cite{hui2024qwen2}, a state-of-the-art, code-native specialist, to test if our data provides a reasoning signal capable of enhancing even expert models. The ability to improve both a safe generalist and a top specialist demonstrates the fundamental and broad applicability of our method. We evaluate all models on LiveCodeBench (Execution)~\cite{jain2024livecodebench} and CruxEval (Input/Output)~\cite{gu2024cruxeval}, benchmarks specifically chosen for their focus on code execution reasoning. We finetune the base models for 10 epochs with a starting LR of 2e-6. More details about the training setup are included in the Appendix.

\subsection{Data Curation and Preparation}
The foundation of our experiments is a large-scale, high-quality dataset generated by our synthesis pipeline.
\begin{enumerate}
    \item \textbf{Concept Sourcing and Scaling:} The pipeline began by ingesting a diverse set of permissively-licensed documents, including those from the "Free Programming Books" source in the StarCoder2-documentation dataset~\cite{starcoder2-doc} and curated lists of programming books. These sources cover a wide range of topics, from core Python language features and data structures to advanced algorithms. This process yielded an initial set of approximately 15,000 deduplicated concepts. From this seed, we hierarchically synthesized a large pool of programming problems: for each concept, we generated multiple distinct instructions, and for each instruction, multiple candidate solutions and unit tests.

    \item \textbf{Verification and Deduplication:} This large, raw set of candidates was passed through our execution-based verification stage (Stage B), resulting in approximately 85,000 code-test pairs that were confirmed to be functionally correct. A final deduplication pass on similar code snippets and test logic yielded our initial master dataset.

    \textbf{CoT Formatting:} Each sample in the master set was formatted into three distinct reasoning structures. A \textbf{forward CoT} sample presents a question like \texttt{Given input X, what is the output?} followed by a trace-narrated rationale. A \textbf{backward CoT} sample asks \texttt{"What input could produce output Y?"} followed by a deductive, trace-grounded rationale, as shown in Figure~\ref{fig:cot-templates}. Finally, a \textbf{bi-directional} sample combines these into a single, multi-turn conversational format.

    \item \textbf{Difficulty-Based Subsetting:} From the master dataset, we created three distinct subsets for our ablation studies, based on different filtering strategies:
    \begin{itemize}
        \item \textbf{Full Set (54k):} This is the complete, verified, and deduplicated dataset, representing the broadest collection of concepts.
        \item \textbf{Model-Derived Difficulty Set (25k):} This high-quality subset was created via a novel filtering strategy: we kept only the problems from the 54k set that an LLM failed to solve correctly. This creates a targeted curriculum focused on the model's specific weaknesses.
        \item \textbf{Content-Rated Difficulty Set (18k):} This set was further refined from the 25k set by using an LLM to rate the conceptual difficulty of each problem, retaining only those rated as "medium" or "hard".
    \end{itemize}
\end{enumerate}
This curation process provides two key dimensions for our experiments: the data curation strategy (54k vs. 25k vs. 18k) and the reasoning direction (forward, backward, or bi-directional).

  


% Define a new column type for centered, wrapping text. Place this right before your table.
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}

\begin{table*}[t!]
    \centering
    \caption{Comprehensive evaluation results across models, datasets, and training configurations on CruxEval and LiveCodeBench-Exec benchmarks. The experiments follow a top-down approach, with the winning configuration
    from one stage used in the next. Best results for each stage are highlighted in \textbf{bold}. And best result across the exp. stages are highlighted in \textbf{grey}.}
    \label{tab:comprehensive-results}
    \resizebox{\textwidth}{!}{%
    % Use the new C column type for the first two columns. Adjust the width as needed.
    \begin{tabular}{C{2.5cm}|C{2.5cm}|l|c|c|c|cc|cc}
    \hline
    \multirow{2}{*}{\textbf{Experiment Stage}} & \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Training Config}} & \multirow{2}{*}{\textbf{Data Subset}} & \multirow{2}{*}{\textbf{Data Config}} &
    \textbf{LiveCodeBench-Exec} &
    \multicolumn{2}{c|}{\textbf{CruxEval Output}} & \multicolumn{2}{c}{\textbf{CruxEval Input}} \\
     & & & & & \textbf{Pass@1} & \textbf{Pass@1} & \textbf{Pass@5} & \textbf{Pass@1} & \textbf{Pass@5} \\
         \hline
    \hline
    % \multicolumn{9}{c}{\textit{Granite-3.3-8B Model Family}} \\
    % \hline
    \parbox{2.5cm}{\centering Baselines} & \parbox{2.5cm}{\centering Granite-3.3-8B-base} & Base (default) & N/A & N/A & 18.3 & 15.5 & 25.3 & 14.3 & 28.4 \\
    \hline
    \multirow{3}{*}{\parbox{2.5cm}{\centering Data Curation}} & \multirow{6}{*}{\parbox{2.5cm}{\centering Granite-3.3-8B (FT)}} & \multirow{3}{*}{Fwd} & 18k & Difficulty-Filt. & 43.5$_{\textcolor{blue}{(+25.2)}}$ & 36.1$_{\textcolor{blue}{(+20.6)}}$ & 58.2$_{\textcolor{blue}{(+32.9)}}$ & 35.8$_{\textcolor{blue}{(+21.5)}}$ & 57.9$_{\textcolor{blue}{(+29.5)}}$ \\
     & & & 25k (best perf.) & Correctness-Filt. & \textbf{44.9}$_{\textcolor{blue}{(+26.6)}}$ & \textbf{42.7}$_{\textcolor{blue}{(+27.2)}}$ & \textbf{64.7}$_{\textcolor{blue}{(+39.4)}}$ & \textbf{40.2}$_{\textcolor{blue}{(+25.9)}}$ &
    \textbf{63.5}$_{\textcolor{blue}{(+35.1)}}$ \\
     & & & 54k & Full Set & 34.1$_{\textcolor{blue}{(+15.8)}}$ & 28.9$_{\textcolor{blue}{(+13.4)}}$ & 55.2$_{\textcolor{blue}{(+29.9)}}$ & 28.8$_{\textcolor{blue}{(+14.5)}}$ & 54.9$_{\textcolor{blue}{(+26.5)}}$ \\
    \cline{1-1}\cline{3-10}
    \multirow{3}{*}{\parbox{2.5cm}{\centering Reasoning Direction}} & & Fwd & 25k & Correctness-Filt. & \cellcolor{highlightgray}{\textbf{44.9}$_{\textcolor{blue}{\textbf{(+26.6)}}}$} & 42.7$_{\textcolor{blue}{(+27.2)}}$ & 64.7$_{\textcolor{blue}{(+39.4)}}$ & 40.2$_{\textcolor{blue}{(+25.9)}}$ & 63.5$_{\textcolor{blue}{(+35.1)}}$ \\
     & & Bwd & 25k & Correctness-Filt. & 35.4$_{\textcolor{blue}{(+17.1)}}$ & 39.3$_{\textcolor{blue}{(+23.8)}}$ & 61.3$_{\textcolor{blue}{(+36.0)}}$ & 41.5$_{\textcolor{blue}{(+27.2)}}$ & 64.8$_{\textcolor{blue}{(+36.4)}}$ \\
     & & Bi-directional & 25k & Correctness-Filt. & 44.3$_{\textcolor{blue}{(+26.0)}}$ & \cellcolor{highlightgray}{\textbf{45.7}$_{\textcolor{blue}{\textbf{(+30.2)}}}$} & \cellcolor{highlightgray}{\textbf{67.4}$_{\textcolor{blue}{\textbf{(+42.1)}}}$} & \cellcolor{highlightgray}{\textbf{42.1}$_{\textcolor{blue}{\textbf{(+27.8)}}}$} & \cellcolor{highlightgray}{\textbf{65.2}$_{\textcolor{blue}{\textbf{(+36.8)}}}$} \\
    \hline
    \hline
    % \multicolumn{9}{c}{\textit{Qwen2.5-7B Model Family}} \\
    % \hline
    \parbox{2.5cm}{\centering Baselines} & \parbox{2.5cm}{\centering Qwen2.5-Coder-7B} & Base (default) & N/A & N/A & 46.3 & 45.3 & 52.12 & 47.5 & 49 \\
    \hline
    \multirow{3}{*}{\parbox{2.5cm}{\centering Data Curation}} & \multirow{6}{*}{\parbox{2.5cm}{\centering Qwen2.5-Coder-7B (FT)}} & \multirow{3}{*}{Fwd} & 18k & Difficulty-Filt. & 66.9$_{\textcolor{blue}{(+20.6)}}$ & 58.4$_{\textcolor{blue}{(+13.1)}}$ & 75.5$_{\textcolor{blue}{(+23.4)}}$ &
    57.2$_{\textcolor{blue}{(+9.7)}}$ & 69.4$_{\textcolor{blue}{(+20.4)}}$ \\
     & & & 25k & Correctness-Filt. & \textbf{67.0}$_{\textcolor{blue}{(+20.7)}}$ & 57.5$_{\textcolor{blue}{(+12.2)}}$ & 73.9$_{\textcolor{blue}{(+21.8)}}$ & 59.8$_{\textcolor{blue}{(+12.3)}}$ & 67.5$_{\textcolor{blue}{(+18.5)}}$ \\
     & & & 54k (best perf.)& Full Set & 66.5$_{\textcolor{blue}{(+20.2)}}$ & \textbf{58.6}$_{\textcolor{blue}{(+13.3)}}$ & \textbf{76.0}$_{\textcolor{blue}{(+23.9)}}$ & \textbf{60.5}$_{\textcolor{blue}{(+13.0)}}$ & \textbf{68.3}$_{\textcolor{blue}{(+19.3)}}$ \\
    \cline{1-1}\cline{3-10}
    \multirow{3}{*}{\parbox{2.5cm}{\centering Reasoning Direction}} & & Fwd & 25k & Correctness-Filt. & 67.0$_{\textcolor{blue}{(+20.7)}}$ & 57.5$_{\textcolor{blue}{(+12.2)}}$ & 73.9$_{\textcolor{blue}{(+21.8)}}$ & 59.8$_{\textcolor{blue}{(+12.3)}}$ & 67.5$_{\textcolor{blue}{(+18.5)}}$ \\
     & & Bwd & 25k & Correctness-Filt. & 57.5$_{\textcolor{blue}{(+11.2)}}$ & 50.4$_{\textcolor{blue}{(+5.1)}}$ & 69.8$_{\textcolor{blue}{(+17.7)}}$ & 61.2$_{\textcolor{blue}{(+13.7)}}$ & 69.1$_{\textcolor{blue}{(+20.1)}}$ \\
     & & Bi-directional & 25k & Correctness-Filt. & \cellcolor{highlightgray}{\textbf{68.2}$_{\textcolor{blue}{\textbf{(+21.9)}}}$} & \cellcolor{highlightgray}{\textbf{59.7}$_{\textcolor{blue}{\textbf{(+14.4)}}}$} & \cellcolor{highlightgray}{\textbf{75.4}$_{\textcolor{blue}{\textbf{(+23.3)}}}$} & \cellcolor{highlightgray}{\textbf{61.9}$_{\textcolor{blue}{\textbf{(+14.4)}}}$} & \cellcolor{highlightgray}{\textbf{70.2}$_{\textcolor{blue}{\textbf{(+21.2)}}}$} \\

    \hline
    \end{tabular}%
    }
    \footnotesize
      \textit{Note:} All Pass@k scores are reported as percentages. Green subscript values show improvement over the Base (Pre-trained) baseline. Abbreviations: Filt. = Filtered, perf. = performing, Fwd: Forward only CoT samples, Bwd: Backward only CoT samples, FT: Supervised fine-tuned.

    \end{table*}

% \subsection{Ablation Study Plan}
% Our experiments are designed as a sequential, top-down filter to find the best data configuration. All experiments are conducted in a Supervised Fine-Tuning (SFT) setting.
% \begin{enumerate}
%     \item \textbf{Data Curation Ablation:} We first aim to identify the optimal data subset. We fine-tune each base model on the 54k, 25k, and 18k datasets using only the forward CoT samples and compare their performance.
%     \item \textbf{Reasoning Direction Ablation:} Using the winning data subset from the previous stage, we then investigate the impact of reasoning format. We fine-tune models on forward-only, backward-only, and the full bi-directional versions of the dataset. \textcolor{blue}{We only used the winning subset considering the fact that backward-only version only replaces forward question-response (q-r) pair with backward q-r pair in each sample, similarly bi-directional version augments each sample by adding backward q-r pair. The input code and test case used for generating questions in each sample remain the same across all the three versions of the data subset (forward, backward and bi-directional). This also helped to reduce the compute requirements by avoiding additional trainings.}
%     \item \textbf{Model Generalization:} By performing this two-stage process on both Granite and Qwen-Coder, we validate the generalization of our findings across different model architectures.
% \end{enumerate}


\subsection{Ablation Study Plan}
Our experiments are designed as a sequential, top-down filter to efficiently identify the optimal data configuration. While testing every combination of our data subsets and reasoning formats would be ideal, such a full factorial experiment is computationally infeasible. Our sequential approach is a practical and methodologically sound alternative that allows us to isolate the impact of our two primary contributions: difficulty-filtered subsets and CoT directionality. We operate on the reasonable assumption that the data subset demonstrating the highest quality for forward CoT will also be the most effective foundation for the other CoT formats. All experiments are conducted in a Supervised Fine-Tuning (SFT) setting.

\begin{enumerate}
    \item \textbf{Data Curation Ablation:} We first aim to identify the optimal data subset. We fine-tune each base model on the 54k, 25k, and 18k datasets using only the forward CoT samples and compare their performance.
    \item \textbf{CoT Direction Ablation:} Using the winning data subset from the previous stage, we then investigate the impact of the CoT format by fine-tuning models on the forward-only, backward-only, and the full bi-directional versions of that dataset.
    \item \textbf{Model Generalization:} By performing this two-stage process on both Granite and Qwen-Coder, we validate the generalization of our findings across different model architectures.
\end{enumerate}





\section{Results and Analysis}
We conduct a series of systematic ablation studies to evaluate the effectiveness of our data synthesis pipeline and determine the optimal training configurations. Our experimental design follows a sequential, top-down filtering approach: we first identify the best-performing data curation strategy, then use that dataset to determine the most effective reasoning direction. This entire process is performed on \texttt{Granite-3.3-8b-base}, and then repeated for \texttt{Qwen2.5-Coder-7B} to validate the generalizability of our findings. We evaluate all models on LiveCodeBench (Execution) and CruxEval (Output and Input prediction).

\subsection{Validation of Dual Agreement Verification}
\label{sec:dual_agreement_validation}
Before presenting the main experimental results, we validate the effectiveness of our Dual Agreement verification approach (Stage B) through a controlled study. This provides empirical evidence for the theoretical justifications presented in Section 3.2.

\subsubsection{Experimental Setup}
We conducted a validation study on 5,000 programming problems for which we had ground-truth correct solutions. For each problem, we systematically varied two parameters: the number of candidate solutions (1, 3, 5, 7, 10) and the number of test cases (3, 5, 7, 10, 15), measuring both consensus scores and alignment with ground truth. This allowed us to quantify how the dual dimensions of solution agreement and test coverage affect verification reliability.

\subsubsection{Consensus Score Scaling with Candidates}
Figure~\ref{fig:consensus_heatmap} shows a heatmap of the percentage of problems achieving high consensus scores (Score $> \tau$ threshold) as a function of the number of solutions (x-axis) and test cases (y-axis). The heatmap reveals a clear pattern: the bottom-right region (high solutions, high tests) shows consistently high consensus rates (70-90\% of problems), while the upper-left region (few solutions, few tests) shows near-zero consensus rates. This validates our hypothesis that \textit{both} multiple diverse solutions and comprehensive test suites are necessary for reliable verification. Notably, increasing solutions from 3 to 10 yields a 3x increase in problems with high consensus scores when test coverage is adequate ($\geq 7$ tests). This empirically confirms the theoretical exponential decay property: more candidates exponentially reduce the probability of false consensus.

\subsubsection{Alignment with Ground Truth}
Figure~\ref{fig:consensus_accuracy} demonstrates that high consensus scores correlate strongly with correctness. We computed the consensus rate (fraction of selected solutions matching ground truth) and plotted it against alignment with benchmark solutions. The results show a monotonic relationship: as consensus rate increases from 20\% to 90\%, alignment with ground truth increases from 30\% to 95\% (with 95\% confidence intervals shown as error bars). This provides empirical evidence that Dual Agreement's scoring mechanism effectively separates correct from incorrect artifacts. Based on this analysis, we set our selection threshold to require minimum cluster size $|C_i| \geq 3$ and test coverage $|T_p| \geq 5$, which yields 92\% alignment with ground truth in our validation set.

\subsubsection{Alternative Verification Strategies}
To contextualize the effectiveness of Dual Agreement, we compare it against four alternative verification strategies for synthetic data generation:

\textbf{Single Execution Verification.} The simplest approach executes each generated solution against a single randomly selected test case. If the solution passes, it is accepted; otherwise, it is rejected. This is fast and deterministic but highly brittle: a single flaky test (due to floating-point precision, randomness, or environmental dependencies) causes false rejections, while a weak test allows buggy solutions through.

\textbf{Formal Verification.} Formal methods use mathematical proofs to guarantee program correctness. Techniques like symbolic execution or SMT solvers can provide ironclad guarantees by exploring all possible execution paths. However, these methods are computationally expensive (often requiring minutes per program), struggle with complex control flow or library calls, and are impractical for large-scale data synthesis (85k problems).

\textbf{LLM-as-Judge.} An alternative is prompting a powerful LLM to analyze the code and judge whether it is correct. This is scalable and flexible but suffers from two critical limitations: (1) \textit{Non-deterministic}: Different sampling runs may yield different judgments, making reproducibility challenging. (2) \textit{Unsound}: The LLM may hallucinate correctness for plausible-looking but incorrect code, lacking grounding in actual execution.

\textbf{CodeT (Outcome-Only Dual Agreement).} CodeT~\cite{chen2022codetcodegenerationgenerated} applies Dual Agreement to verify that generated \textit{code solutions} pass tests, achieving high accuracy through statistical consensus. However, it only verifies the final outcome, not the reasoning process. We extend this principle by generating CoT rationales from execution traces, thereby verifying the \textit{reasoning steps themselves}.

\textbf{Comparison Results.} Table~\ref{tab:verification_comparison} compares these approaches on our 5k validation set. Dual Agreement achieves 92\% accuracy while remaining scalable and deterministic, substantially outperforming single execution (67\%) and LLM-as-Judge (74\%), while being practical at scale unlike formal verification. Figure~\ref{fig:consensus_heatmap} and Figure~\ref{fig:consensus_accuracy} provide detailed breakdowns showing how consensus scores scale with candidate diversity and correlate with ground-truth correctness.

\begin{table}[h!]
\centering
\caption{Comparison of verification methods on 5k validation set. Dual Agreement achieves high accuracy while remaining scalable and deterministic.}
\label{tab:verification_comparison}
\begin{tabular}{lccc}
\toprule
\textbf{Verification Method} & \textbf{Accuracy} & \textbf{Scalable} & \textbf{Deterministic} \\
\midrule
Single Execution & 67\% & \cmark & \cmark \\
LLM-as-Judge & 74\% & \cmark & \xmark \\
CodeT (Outcome Only) & 89\% & \cmark & \cmark \\
Dual Agreement (Ours) & \textbf{92\%} & \cmark & \cmark \\
Formal Verification & $>$95\% & \xmark & \cmark \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Impact of Verification Approach on Fine-Tuning}
To understand how different verification methods affect downstream model performance, we conducted an ablation study where we fine-tuned models on data generated with different verification approaches. Using the same 25k problem set, we generated training data under six conditions: (1) \textbf{Base Model}: No fine-tuning, (2) \textbf{Few-Shot}: No fine-tuning, using 3-shot prompting with example problems, (3) \textbf{Q\&A Only}: Fine-tuning with question-answer pairs only—no CoT rationales in the training data, (4) \textbf{LLM-Generated CoT}: Fine-tuning with CoT rationales that are purely LLM-generated without execution grounding, (5) \textbf{Ours (Minimal Execution)}: Our trace-grounded CoT approach using only the first generated solution and first test case (1 solution, 1 test), and (6) \textbf{Our Full Approach}: Our complete pipeline with Dual Agreement verification (5 solutions, 10 tests) and trace-grounded rationales.

Table~\ref{tab:verification_ablation} presents the results. Our full approach achieves the highest performance across all benchmarks, with the \texttt{Qwen2.5-Coder-7B} model reaching 68.2\% on LiveCodeBench, 59.7\% on CruxEval Output, and 61.9\% on CruxEval Input. The minimal execution variant (64.1\% LiveCodeBench, -4.1 points) demonstrates that verification quality degrades without sufficient candidate diversity, confirming the value of Dual Agreement's statistical consensus. The LLM-Generated CoT approach performs substantially worse (59.3\% LiveCodeBench, -8.9 points), demonstrating that execution grounding is essential—LLM-generated explanations lack the factual accuracy guaranteed by trace translation. Notably, removing rationales entirely (Q\&A Only) causes a severe performance drop (52.7\% LiveCodeBench, -15.5 points), validating that trace-grounded reasoning chains provide crucial learning signal beyond input-output mappings. The few-shot baseline (48.1\%, 47.0\%, 48.8\%) performs only marginally better than the base model (46.3\%, 45.3\%, 47.5\%), highlighting the value of fine-tuning on verified data.

\begin{table}[h!]
\centering
\caption{Impact of verification approach on fine-tuning results. All models fine-tuned on 25k bi-directional data with different verification methods. Tested on \texttt{Qwen2.5-Coder-7B}.}
\label{tab:verification_ablation}
\resizebox{\columnwidth}{!}{
\begin{tabular}{lcccc}
\toprule
\textbf{Verification Approach} & \textbf{LiveCodeBench} & \textbf{CruxEval-O} & \textbf{CruxEval-I} & \textbf{$\Delta$ from Base} \\
\midrule
Base Model (No FT) & 46.3 & 45.3 & 47.5 & -- \\
Few-Shot (3 examples) & 48.1 & 47.0 & 48.8 & +1.8 \\
\midrule
Q\&A Only (No CoT) & 52.7 & 50.2 & 51.3 & +6.4 \\
LLM-as-Judge + LLM CoT & 59.3 & 54.1 & 55.7 & +13.0 \\
Dual Agree. (Small: 2S/3T) & 64.1 & 56.8 & 58.4 & +17.8 \\
\midrule
\textbf{Our Approach (5S/10T + Trace)} & \textbf{68.2} & \textbf{59.7} & \textbf{61.9} & \textbf{+21.9} \\
\bottomrule
\end{tabular}
}
\end{table}

These results validate that Dual Agreement provides a practical, scalable, and empirically reliable verification method for large-scale synthetic data generation, and that verification quality directly impacts downstream model capabilities.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\columnwidth]{instruct_boost_single.png}
    \caption{Performance boost from fine-tuning instruct models. Solid bars represent the baseline instruct models; hatched bars show the improvement after fine-tuning with the best-performing 25k bi-directional subset of data.}
    \label{fig:instruct-boost}
    \vspace{-10pt}
\end{figure}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\columnwidth]{sota_comparison_final.png}
    \caption{Performance comparison against SOTA baselines~\cite{ding2024semcodertrainingcodelanguage} Our model's results are highlighted with a hatch pattern. \textit{Abbreviations:} CL: CodeLlama, SC2: StarCoder2, DSCoder: DeepSeekCoder, MCoder: MagicCoder, -Py: Python, -Inst: Instruct.}
    \label{fig:sota-comparison}
    \vspace{-12pt}
\end{figure}
The comprehensive results of these experiments are presented in Table \ref{tab:comprehensive-results}.

\subsection{Impact of Data Curation}
Our first primary ablation investigates the impact of data quality and size by fine-tuning models on three forward-only data subsets: a complete 54k sample set, a higher-quality 25k set filtered for correctness, and a challenging 18k set filtered for difficulty.

For the \texttt{Granite-3.3-8b} model, the results are definitive. As shown in Table \ref{tab:comprehensive-results}, the \textbf{25k correctness-filtered dataset} substantially outperforms the others across all benchmarks. On LiveCodeBench, it achieves a score of 44.9\%, an absolute gain of \textbf{+26.6} over the base model, and significantly outperforms the larger 54k set (34.1\%). This powerful improvement from a targeted, high-quality dataset strongly indicates that for complex reasoning, data verifiability is far more impactful than sheer volume.

When repeating the experiment with \texttt{Qwen2.5-Coder-7B}, all three fine-tuned models dramatically outperform the base model. For instance, the 25k dataset boosts the LiveCodeBench score from 46.3\% to 67.0\% (\textbf{+20.7}). Unlike with Granite, the performance across the three data subsets is highly competitive. The 25k dataset achieves the highest score on LiveCodeBench, while the 54k dataset has a slight edge on the CruxEval benchmarks. Given the consistently strong performance of the 25k set, we selected it as the winning configuration for all subsequent experiments.



\subsection{Impact of Reasoning Direction}
Using the winning 25k dataset from the previous stage, we evaluated the impact of our novel bi-directional data format. We compared models trained on forward-only, backward-only, and the complete bi-directional datasets.

For the \texttt{Granite-3.3-8b} model, the results in Table \ref{tab:comprehensive-results} highlight the benefits of bi-directional training. The model trained on the \textbf{bi-directional dataset} achieves the highest scores on both CruxEval Output (45.7\% pass@1) and Input (42.1\% pass@1). This represents a final performance gain of \textbf{+30.2} on CruxEval Output over the base model, demonstrating that teaching cause-to-effect and effect-to-cause reasoning provides a synergistic improvement.

This finding is further confirmed with the \texttt{Qwen2.5-Coder-7B} model. The bi-directionally trained model once again emerges as the top performer, achieving the best results on LiveCodeBench (68.2\%) and CruxEval Input (61.9\% pass@1). This configuration's final score on LiveCodeBench represents a \textbf{+21.9} point gain over the base model. To contextualize this performance, Figure \ref{fig:sota-comparison} compares our best model against a range of competitive foundation models. Our fine-tuned model establishes a leading score on LiveCodeBench-Execution and demonstrates highly competitive performance across all reasoning benchmarks.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.7\linewidth]{clean_best_plots.png}
    \caption[CoT quality analysis]{Chain-of-Thought quality analysis. Top: CoT-to-outcome consistency vs reasoning length with regression analysis. Bottom: Information content distributions. Verified CoT training shows superior consistency scaling ($R^2 = 0.122$ vs $0.011$) and 761\% higher information richness ($d = 7.93$, $p < 0.001$).}

    \label{fig:cot-quality}
\end{figure}

\subsection{Enhancing Instruction-Tuned Models}
To confirm that our best-performing bi-directional data provides a specialized signal complementary to general instruction tuning, we fine-tuned existing instruct-tuned models. As shown in Figure \ref{fig:instruct-boost}, this provides a substantial boost, yielding a massive \textbf{+39.9} gain on CruxEval Input for \texttt{granite-3.3-8B-instruct} and a \textbf{+21.5} gain on CruxEval Output for the already strong \texttt{Qwen2.5-Coder-7B-Instruct}, demonstrating the value of our execution-grounded data.





\subsection{Chain-of-Thought Consistency and Quality Analysis}

  To assess the quality of reasoning generated by our fine-tuned model, we compare CoTs generated by our \texttt{Qwen2.5-Coder-7B-FT} verified CoT model against the base \texttt{Qwen2.5-Coder-7B} model on CruxEval-O problems. For each problem, we generate CoT from both models and evaluate two key dimensions: \textbf{consistency} between reasoning and final answer, and
  \textbf{information content} of the reasoning content.

  \textbf{Consistency Analysis.} We measure consistency using a composite metric that evaluates reasoning-answer alignment across multiple dimensions: entailment patterns, conceptual
  overlap, and sequential coherence. Figure~\ref{fig:cot-quality} (top) shows our model exhibits strong correlation between CoT length and consistency ($R^2 = 0.122$), indicating longer reasoning is more logically coherent. The baseline model shows no such relationship ($R^2 = 0.011$). Moreover, when CoT is consistent with the final answer, our model is more likely to produce correct solutions (AUC = 0.567) compared to the baseline where consistency poorly predicts correctness (AUC = 0.502).

  \textbf{Information Richness.} The bottom plot reveals that our model produces substantially more informative reasoning. Using an entropy-based metric that accounts for vocabulary diversity and technical term density, we find the CoTs from our fine-tuned model are \textbf{761\% more information-rich} than those from the baseline. This difference is statistically significant with a very large effect size (Cohen's $d = 7.93$, $p < 0.001$), confirming that our training method generates more detailed and semantically meaningful rationales.
  
  These results confirm that verified CoT training fundamentally enhances reasoning quality, producing more consistent and informative chain-of-thought processes that better support
  final outputs.

% \subsection{Chain-of-Thought Quality Analysis}
% To validate that our data improves not just accuracy but the reasoning process itself, we analyze the quality of the CoTs generated by our fine-tuned Qwen2.5 model versus those from the baseline Qwen2.5 model for problems in the CruxEval-O benchmark. We compare the CoTs along two axes: their internal consistency with the final answer and their information richness.

% \textbf{Consistency Analysis.} As shown in the top plot of Figure~\ref{fig, we find that the reasoning from our fine-tuned model is far more reliable. We use a composite score that measures consistency from multiple dimensions, including \textbf{entailment} (logical connectors near the answer), \textbf{conceptual alignment} (concept overlap), and \textbf{sequential coherence}. Our fine-tuned model's CoTs show a clear positive correlation between length and consistency ($R^2 = 0.122$), indicating that longer rationales build logically toward the conclusion. The baseline model's CoTs show no such relationship ($R^2 = 0.011$). Furthermore, the consistency score of our model's CoT is a much better predictor of a correct final answer, achieving a superior Area Under the ROC Curve (AUC) of 0.567 compared to the baseline's near-random 0.502.

% \textbf{Information Richness.} The bottom plot of Figure~\ref{fig:cot-quality} reveals that our model produces substantially more informative reasoning. Using an entropy-based metric that accounts for vocabulary diversity and technical term density, we find that the CoTs from our fine-tuned model are \textbf{761\% more information-rich} than those from the baseline. This difference is statistically significant with a very large effect size (Cohen's $d = 7.93$, $p < 0.001$), confirming that our training method generates more detailed and semantically meaningful rationales.
  
% These results confirm that verified CoT training fundamentally enhances reasoning quality, producing more consistent and informative chain-of-thought processes that better support
%   final outputs.  
%   % These findings demonstrate that verified CoT training not only improves accuracy but fundamentally enhances the quality of reasoning by producing more consistent, information-rich, and logically coherent chain-of-thought processes that better support the final model outputs through explicit logical connectors, conceptual alignment, and sequential coherence.



% \FloatBarrier
\section{Conclusion and Future Work}
We introduced a methodology for generating verifiable CoT data for code reasoning, addressing the challenge of logical integrity in synthetic datasets. By grounding every reasoning step in program execution traces, our pipeline eliminates logical hallucinations and produces high-fidelity training data that is correct by construction.

Our systematic evaluation confirms the effectiveness of this approach. We show that verified data quality is more important than quantity and that our novel bi-directional format significantly improves reasoning. These findings hold across different model architectures, where fine-tuning boosted performance by as much as \textbf{+30.2 points} on the CruxEval-O reasoning benchmark, underscoring the fundamental impact of our contribution.

Future work includes extending our language-agnostic pipeline to other languages like C++ and Java and exploring its use in advanced training schemes such as offline reinforcement learning with DPO. To aid the development of more robust language models, we will publicly release our complete synthesis pipeline.

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


%%
%% If your work has an appendix, this is the place to put it.
\appendix



\end{document}
\endinput
%%
%% End of file `sample-acmsmall-conf.tex'.
